%Este trabalho está licenciado sob a Licença Atribuição-CompartilhaIgual 4.0 Internacional Creative Commons. Para visualizar uma cópia desta licença, visite http://creativecommons.org/licenses/by-sa/4.0/deed.pt_BR ou mande uma carta para Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.

\chapter{Métodos para Sistemas Lineares}\label{cap_sislin}
\thispagestyle{fancy}

Neste capítulo, discutimos sobre métodos diretos para a resolução de sistemas lineares de $n$-equações com $n$-incógnitas. Isto é, sistemas que podem ser escritos na seguinte \hl{\emph{forma algébrica}}
\begin{align}
  a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n &= b_1\\
  a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n &= b_2\\
  &\vdots \\
  a_{n1}x_1 + a_{n2}x_2 + \cdots + a_{nn}x_n &= b_n.
\end{align}
Equivalentemente, o sistema pode ser escrito na \hl{\emph{forma matricial}}
\begin{equation}\hleq
  A\pmb{x} = \pmb{b}
\end{equation}
onde, $A = [a_{i,j}]_{i,j=1}^{n,n}$ é a \hl{\emph{matriz dos coeficientes}}
\begin{equation}
  A =
  \begin{bmatrix}
    a_{1,1} & a_{1,2} & \ldots & a_{1,n}\\
    a_{2,1} & a_{2,2} & \ldots & a_{2,n}\\
    \vdots & \vdots & \ddots & \vdots\\
    a_{n,1} & a_{n,2} & \ldots & a_{n,n}\\
  \end{bmatrix},
\end{equation}
o \hl{\emph{vetor das incógnitas}} $\pmb{x} = (x_i)_{i=1}^n$ é
\begin{equation}
  \pmb{x} =
  \begin{bmatrix}
    x_1\\
    x_2\\
    \vdots\\
    x_n
  \end{bmatrix}
\end{equation}
e o \hl{\emph{vetor dos termos constantes}} $\pmb{b} = (b_i)_{i=1}^n$ é
\begin{equation}
  \pmb{b} =
  \begin{bmatrix}
    b_1\\
    b_2\\
    \vdots\\
    b_n
  \end{bmatrix}
\end{equation}

\section{Método da Decomposição LU}\label{cap_sislin_sec_lu}

O Método da Decomposição LU é uma forma eficiente de se resolver sistemas lineares de pequeno porte. \hl{Dado um sistema $A\pmb{x} = \pmb{b}$, a ideia é decompor a matriz $A$ como o produto de uma \emph{matriz triangular inferior} $L$ (do inglês, \textit{lower triangular matrix}) com uma \emph{matriz triangular superior} $U$ (do inglês, \textit{upper triangular matrix})}, i.e.
\begin{equation}
  \hleq A = LU.
\end{equation}
Com isso, o sistema pode ser reescrito na forma
\begin{gather}
  A\pmb{x} = \pmb{b}\\
  (LU)\pmb{x} = \pmb{b}\\
  L(U\pmb{x}) = \pmb{b}.
\end{gather}
Denotando,
\begin{equation}
  U\pmb{x} =: \emph{y}
\end{equation}
podemos resolver o seguinte sistema triangular
\begin{equation}
  L\pmb{y} = \pmb{b}.
\end{equation}
Tendo resolvido este sistema, a solução do sistema $A\pmb{x} = \pmb{b}$ pode, então, ser computada como a solução do sistema triangular
\begin{equation}
  U\pmb{x} = \pmb{y}.
\end{equation}
Ou seja, \hl{a decomposição LU nos permite resolver uma sistema pela resolução de dois sistemas triangulares}.

\subsection{Sistemas Triangulares}

Antes de estudarmos como podemos computar a decomposição LU de uma matriz, vamos discutir sobre a resolução de sistemas triangulares.

\subsubsection{Sistema Triangular Inferior}

\hl{Um \emph{sistema linear triangular inferior}} tem a forma algébrica
\begin{equation}
  \begin{matrix}
    a_{1,1}x_1 &&&&&= b_1\\
    a_{2,1}x_1 &+ a_{2,2}x_2 &&&&= b_2\\
    \vdots &&&&& \vdots\\
    a_{n-1,1}x_1 &+ a_{n-1,2}x_2 &+ \cdots &+ a_{n-1,n-1}x_{n-1} &&= b_{n-1}\\
    a_{n,1}x_1 &+ a_{n,2}x_2 &+ \cdots &+ a_{n,n-1}x_{n-1} &+ a_{n,n}x_n &= b_n
  \end{matrix}
\end{equation}
\hl{Pode ser diretamente resolvido de cima para baixo}, i.e.
\begin{align}
  x_1 &= \frac{b_1}{a_{1,1}}\\
  x_2 &= \frac{b_{2} - a_{2,1}x_1}{a_{2,2}}\\
      &\vdots\\
  x_n &= \frac{b_n - a_{n,1}x_1 - a_{n,2}x_2 - \ldots - a_{n,n-1}x_{n-1}}{a_{n,n}}
\end{align}

\begin{ex}
  Vamos resolver o sistema triangular inferior
  \begin{equation}
    \begin{matrix}
      x_1 &&&= 2\\
      -3x_1 &+ 2x_2 &&= -8\\
      -x_1 &+ x_2 &- x_3 &= 0
    \end{matrix}
  \end{equation}
  Na primeira equação, temos
  \begin{equation}
    x_1 = 2
  \end{equation}
  Então, da segunda equação do sistema
  \begin{gather}
    -3x_1 + 2x_2 = -8\\
    x_2 = \frac{-8 + 3x_1}{2}\\
    x_2 = \frac{-8 + 3\cdot 2}{2}\\
    x_2 = -1
  \end{gather}
  E, por fim, da última equação
  \begin{gather}
    -x_1 + x_2 - x_3 = 0\\
    x_3 = \frac{x_1 - x_2}{-1}\\
    x_3 = \frac{2 - (-1)}{-1}\\
    x_3 = 3
  \end{gather}
  Concluímos que a solução do sistema é $\pmb{x} = (2, -1, 3)$.

\begin{lstlisting}[caption=solSisTriaInf.py, label=cap_sislin_sec_lu:cod:solSisTriaInf]
import numpy as np

def solSisTriaInf(A, b):
    n = b.size
    x = np.zeros_like(b)
    for i in range(n):
        x[i] = b[i]
        for j in range(i):
            x[i] -= A[i,j]*x[j]
        x[i] /= A[i,i]
    return x
    

# mat coefficientes
A = np.array([[1., 0., 0.],
              [-3., 2., 0.],
              [-1., 1., -1.]])

# vet termos constantes
b = np.array([2., -8., 0.])

# resol sis lin
x = solSisTriaInf(A, b)
print(x)
\end{lstlisting}
\end{ex}

\begin{obs}\normalfont{(\hl{Número de operações em ponto flutuante}.)}
  A computação da solução de um sistema $n\times n$ triangular inferior requer $O(n^2)$ operações em ponto flutuante (multiplicações/divisões e adições/subtrações).
\end{obs}

\subsubsection{Sistema Triangular Superior}

\hl{Um \emph{sistema linear triangular superior}} tem a forma algébrica
\begin{equation}
  \begin{matrix}
    a_{1,1}x_1 &+ a_{1,2}x_2 &+ \cdots &+ a_{1,n}x_n &= b_1\\
              &a_{2,2}x_2  &+ \cdots &+ a_{2,n}x_{n} &= b_2\\
              &&& \vdots  &\vdots \\
              &&& a_{n,n}x_n &= b_n
  \end{matrix}
\end{equation}
\hl{Pode ser diretamente resolvido de baixo para cima}, i.e.
\begin{align}
  x_n &= \frac{b_n}{a_{n,n}}\\
  x_{n-1} &= \frac{b_{n-1} - a_{n-1,n}x_n}{a_{n-1,n-1}}\\
      &\vdots\\
  x_n &= \frac{b_1 - a_{1,1}x_1 - a_{1,2}x_2 - \ldots - a_{1,n-1}x_{n-1}}{a_{1,1}}
\end{align}

\begin{ex}
  Vamos resolver o sistema triangular superior
  \begin{equation}
    \begin{matrix}
      2x_1 &- x_2 &+ 2x_3 &= 7\\
      & 2x_2 &- x_3 &= -3\\
      &&3x_3 &= 3
    \end{matrix}
  \end{equation}
  Da última equação, temos
  \begin{gather}
    x_3 = \frac{3}{3}\\
    x_3 = 1
  \end{gather}
  Então, da segunda equação do sistema, obtemos
  \begin{gather}
    2x_2 - x_3 = -3\\
    x_2 = \frac{x_3 - 3}{2}\\
    x_2 = \frac{1 - 3}{2}\\
    x_2 = -1
  \end{gather}
  Por fim, da primeira equação
  \begin{gather}
    2x_1 - x_2 + 2x_3 = 7\\
    x_1 = \frac{7 + x_2 - 2x_3}{2}\\
    x_1 = \frac{7 - 1 - 2}{2}\\
    x_1 = 2
  \end{gather}
  Concluímos que a solução do sistema é $\pmb{x} = (2, -1, 1)$.

\begin{lstlisting}[caption=solSisTriaSup.py, label=cap_sislin_sec_lu:cod:solSisTriaSup]
import numpy as np

def solSisTriaSup(A, b):
    n = b.size
    x = np.zeros_like(b)
    for i in range(n-1,-1,-1):
        x[i] = b[i]
        for j in range(n-1,i,-1):
            x[i] -= A[i,j]*x[j]
        x[i] /= A[i,i]
    return x
    

# mat coefficientes
A = np.array([[2., -1., 2.],
              [0., 2., -1.],
              [0., 0., 3.]])

# vet termos constantes
b = np.array([7., -3., 3.])

# sol sis lin
x = solSisTriaSup(A, b)
print(x)
\end{lstlisting}
\end{ex}

\begin{obs}\normalfont{(\hl{Número de operações em ponto flutuante}.)}
  A computação da solução um sistema $n\times n$ triangular superior requer $O(n^2)$ operações em ponto flutuante (multiplicações/divisões e adições/subtrações).
\end{obs}

\subsection{Decomposição LU}

\hl{O procedimento da decomposição LU é equivalente ao método de eliminação gaussiana}. Consideramos uma matriz $A = [a_{ij}]_{i,j=1}^{n,n}$, com $a_{1,1}\neq 0$, e começamos denotando esta matriz por $U^{(0)} = [u_{i,j}^{(0)}]_{i,j=1}^{n,n} = A$ e tomando $L^{(0)} = I_{n\times n}$. A eliminação abaixo do pivô $u_{1,1}^{(0)}$, pode ser computada com as seguintes operações equivalentes por linha
\begin{equation}
  U^{(0)} = \begin{bmatrix}
    u_{1,1}^{(0)} & u_{1,2}^{(0)} & u_{1,3}^{(0)} & \ldots & u_{1,n}^{(0)} \\
    u_{2,1}^{(0)} & u_{2,2}^{(0)} & u_{2,3}^{(0)} & \ldots & u_{2,n}^{(0)} \\
    u_{3,1}^{(0)} & u_{3,2}^{(0)} & u_{3,3}^{(0)} & \ldots & u_{3,n}^{(0)} \\
    \vdots & \vdots & \vdots & \ldots & \vdots \\
    u_{n,1}^{(0)} & u_{n,2}^{(0)} & a_{n,3}^{(0)} & \ldots & u_{n,n}^{(0)}
  \end{bmatrix}
  \begin{array}{l}
    U_1^{(1)} \leftarrow U_1^{(0)}\\
    U_2^{(1)} \leftarrow U_2^{(0)} - l_{2,1}^{(1)}U_1^{(0)}\\
    U_3^{(1)} \leftarrow U_3^{(0)} - l_{3,1}^{(1)}U_1^{(0)}\\
    \vdots\\
    U_n^{(1)} \leftarrow U_n^{(0)} - l_{n,1}^{(1)}U_1^{(0)}
  \end{array}
\end{equation}
onde, $l_{i,1}^{(1)}=u_{i,1}^{(0)}/u_{1,1}^{(0)}$, $i=2, 3, \dotsc, n$.

Destas computações, obtemos uma nova matriz da forma
\begin{equation}
  U^{(1)} = \begin{bmatrix}
    u_{1,1}^{(1)} & u_{1,2}^{(1)} & u_{1,3}^{(1)} & \ldots & u_{1,n}^{(1)} \\
    0 & u_{2,2}^{(1)} & u_{2,3}^{(1)} & \ldots & u_{2,n}^{(1)} \\
    0 & u_{3,2}^{(1)} & u_{3,3}^{(1)} & \ldots & u_{3,n}^{(1)} \\
    \vdots & \vdots & \vdots & \ldots & \vdots \\
    0 & u_{n,2}^{(1)} & u_{n,3}^{(1)} & \ldots & u_{n,n}^{(1)}
  \end{bmatrix}  
\end{equation}
E, denotando
\begin{equation}
  L^{(1)} =
  \begin{bmatrix}
    1 & 0 & 0 & \ldots & 0\\
    l_{2,1}^{(1)} & 1 & 0 & \ldots & 0\\
    l_{3,1}^{(1)} & 0 & 1 & \ldots & 0\\
    \vdots & \vdots & \vdots & \ldots & \vdots\\
    l_{n,1}^{(1)} & 0 & 0 & \ldots & 1
  \end{bmatrix}
\end{equation}
temos
\begin{equation}
  A = L^{(1)}U^{(1)}.
\end{equation}
No caso de $u^{(1)}_{2,2}\neq 0$, podemos continuar com o procedimento de eliminação gaussiana com as seguintes operações equivalentes por linha
\begin{equation}
  U^{(1)} = \begin{bmatrix}
    u_{1,1}^{(1)} & u_{1,2}^{(1)} & u_{1,3}^{(1)} & \ldots & u_{1,n}^{(1)} \\
    0 & u_{2,2}^{(1)} & u_{2,3}^{(1)} & \ldots & u_{2,n}^{(1)} \\
    0 & u_{3,2}^{(1)} & u_{3,3}^{(1)} & \ldots & u_{3,n}^{(1)} \\
    \vdots & \vdots & \vdots & \ldots & \vdots \\
    0 & u_{n,2}^{(1)} & u_{n,3}^{(1)} & \ldots & u_{n,n}^{(1)}
  \end{bmatrix}  
  \begin{array}{l}
    U_1^{(2)}\leftarrow U_1^{(1)}\\
    U_2^{(2)}\leftarrow U_2^{(1)}\\
    U_3^{(2)} \leftarrow U_3^{(1)} - l_{3,2}^{(2)}U_2^{(1)}\\
    \vdots\\
    U_n^{(2)} \leftarrow U_n^{(1)} - l_{n,2}^{(2)}U_2^{(1)}\\
  \end{array}
\end{equation}
onde, $l_{i,2}^{(2)} = u_{i,2}^{(1)}/u_{2,2}^{(1)}$, $i=3, 4, \ldots, n$. Isto nos fornece
o que nos fornece
\begin{equation}
  U^{(2)} = \begin{bmatrix}
    u_{1,1}^{(2)} & u_{1,2}^{(2)} & u_{1,3}^{(2)} & \ldots & u_{1,n}^{(2)} \\
    0 & u_{2,2}^{(2)} & u_{2,3}^{(2)} & \ldots & u_{2,n}^{(2)} \\
    0 & 0 & u_{3,3}^{(2)} & \ldots & u_{3,n}^{(2)} \\
    \vdots & \vdots & \vdots & \ldots & \vdots \\
    0 & 0 & u_{n,3}^{(2)} & \ldots & u_{n,n}^{(2)}
  \end{bmatrix}.
\end{equation}
Além disso, denotando
\begin{equation}
  L^{(2)} =
  \begin{bmatrix}
    1 & 0 & 0 & \ldots & 0\\
    l_{2,1}^{(1)} & 1 & 0 & \ldots & 0\\
    l_{3,1}^{(1)} & l_{3,2}^{(2)} & 1 & \ldots & 0\\
    \vdots & \vdots & \vdots & \ldots & \vdots\\
    l_{n,1}^{(1)} & l_{n,2}^{(2)} & 0 & \ldots & 1
  \end{bmatrix}
\end{equation}
temos
\begin{equation}
  A = L^{(2)}U^{(2)}.
\end{equation}
Continuando com este procedimento, ao final de $n-1$ passos teremos obtido a decomposição
\begin{equation}
  A = LU,
\end{equation}
onde $L$ é a matriz triangular inferior
\begin{equation}
  L = L^{(n-1)} =   \begin{bmatrix}
    1 & 0 & 0 & \ldots & 0\\
    l_{2,1}^{(1)} & 1 & 0 & \ldots & 0\\
    l_{3,1}^{(1)} & l_{3,2}^{(2)} & 1 & \ldots & 0\\
    \vdots & \vdots & \vdots & \ldots & \vdots\\
    l_{n,1}^{(1)} & l_{n,2}^{(2)} & l_{n,3}^{(3)} & \ldots & 1
  \end{bmatrix}
\end{equation}
e $U$ é a matriz triangular superior
\begin{equation}
  U = U^{(n-1)} = \begin{bmatrix}
    u_{1,1}^{(0)} & u_{1,2}^{(0)} & u_{1,3}^{(0)} & \ldots & u_{1,n}^{(0)} \\
    0 & u_{2,2}^{(1)} & u_{2,3}^{(1)} & \ldots & u_{2,n}^{(1)} \\
    0 & 0 & u_{3,3}^{(2)} & \ldots & u_{3,n}^{(2)} \\
    \vdots & \vdots & \vdots & \ldots & \vdots \\
    0 & 0 & 0 & \ldots & u_{n,n}^{(n-1)}
  \end{bmatrix}.
\end{equation}

\begin{ex}\label{ex:lu}
  Consideramos a seguinte matriz
  \begin{equation}
    A =
    \begin{bmatrix}
      -1 & 2 & -2\\
      3 & -4 & 1\\
      1 & -5 & 3
    \end{bmatrix}.
  \end{equation}
  Para obtermos sua decomposição $LU$ começamos com
  \begin{align}
    L^{(0)} &:=
    \begin{bmatrix}
      1 & 0 & 0\\
      0 & 1 & 0\\
      0 & 0 & 1
    \end{bmatrix},\\
    U^{(0)} &:=
    \begin{bmatrix}
      -1 & 2 & -2\\
      3 & -4 & 1\\
      1 & -5 & 3
    \end{bmatrix}.
  \end{align}
  Então, observando que a eliminação abaixo do pivô $u_{1,1}=-1$ pode ser feita com as seguintes operações equivalentes por linha
  \begin{equation}
    U^{(0)} =
    \begin{bmatrix}
      -1 & 2 & -2\\
      3 & -4 & 1\\
      1 & -5 & 3
    \end{bmatrix}
    \begin{array}{l}
      U_1^{(1)}\leftarrow U_1^{(0)}\\
      U_2^{1}\leftarrow U_2^{(0)} - \frac{3}{-1}U_1^{(0)}\\
      U_3^{1}\leftarrow U_3^{(0)} - \frac{1}{-1}U_1^{(0)}
    \end{array},
  \end{equation}
  temos
  \begin{align}
    L^{(1)} &:=
    \begin{bmatrix}
      1 & 0 & 0\\
      -3 & 1 & 0\\
      -1 & 0 & 1
    \end{bmatrix},\\
    U^{(1)} &:=
    \begin{bmatrix}
      -1 & 2 & -2\\
      0 & 2 & -5\\
      0 & -3 & 1
    \end{bmatrix}.
  \end{align}
  Agora, para eliminarmos abaixo do pivô $u_{2,2}=2$, usamos as operações
  \begin{equation}
    U^{(1)} :=
    \begin{bmatrix}
      -1 & 2 & -2\\
      0 & 2 & -5\\
      0 & -3 & 1
    \end{bmatrix}
    \begin{array}[]l
      U_1^{(2)}\leftarrow U_1^{(1)} \\
      U_2^{(2)}\leftarrow U_2^{(1)} \\
      U_3^{(2)}\leftarrow U_3^{(1)} - \frac{-3}{2}U_2^{(1)}
    \end{array},
  \end{equation}
  donde
  \begin{align}
    L^{(2)} =
    \begin{bmatrix}
      1 & 0 & 0\\
      -3 & 1 & 0\\
      -1 & -1,5 & 1
    \end{bmatrix},\\
    U^{(1)} =
    \begin{bmatrix}
      -1 & 2 & -2\\
      0 & 2 & -5\\
      0 & 0 & -6,5
    \end{bmatrix}.
  \end{align}
Isso completa a decomposição, sendo $L := L^{(3)}$ e $U := U^{(3)}$.

\begin{lstlisting}[caption=lu.py, label='cap_sislin_sec_lu:cod:lu']
import numpy as np

def lu(A):
    # num. de linhas
    n = A.shape[0]

    # inicialização
    U = A.copy()
    L = np.eye(n)

    # decomposição
    for i in range(n-1):
        for j in range(i+1,n):
            L[j,i] = U[j,i]/U[i,i]
            U[j,i:] -= L[j,i]*U[i,i:]

    return L, U

# matriz
A = np.array([[-1., 2., -2.],
              [3., -4., 1.],
              [1., -5., 3.]])

L, U = lu(A)
\end{lstlisting}
\end{ex}

\begin{obs}\normalfont{(\hl{Número de operações em ponto flutuante}.)}
  A decomposição LU de um sistema $n\times n$ requer $O(n^3)$ operações em ponto flutuante (multiplicações/divisões e adições/subtrações).
\end{obs}


\subsection{Resolução do Sistema com Decomposição LU}

Consideramos o sistema linear
\begin{equation}
  A\pmb{x} = \pmb{b}.
\end{equation}

Para resolvê-lo com o \hl{Método LU}, fazemos
\begin{enumerate}[1.]
\item \hl{Computamos a decomposição LU}
  \begin{equation}\hleq
    A = LU.
  \end{equation}
\item \hl{Resolvemos o sistema triangular inferior}
  \begin{equation}\hleq
    L\pmb{y} = \pmb{b}.
  \end{equation}
\item \hl{Resolvemos o sistema triangular superior}
  \begin{equation}\hleq
    U\pmb{x} = \pmb{y}.
  \end{equation}
\end{enumerate}

\begin{ex}\label{ex:lu}
  Vamos resolver o seguinte sistema linear
  \begin{align}
    -x_1 + 2x_2 - 2x_3 &= 6\\
    3x_1 - 4x_2 + x_3 &= -11\\
    x_1 - 5x2 + 3x_3 &= -10.
  \end{align}
  No exemplo anterior (Exemplo \ref{ex:lu}), vimos que a matriz de coeficientes $A$ deste sistema admite a seguinte decomposição LU
  \begin{equation}
    \underbrace{\begin{bmatrix}
      -1 & 2 & -2\\
      3 & -4 & 1\\
      1 & -5 & 3
    \end{bmatrix}}_{A} =
  \underbrace{\begin{bmatrix}
      1 & 0 & 0\\
      -3 & 1 & 0\\
      -1 & -1,5 & 1
    \end{bmatrix}}_{L}
  \underbrace{\begin{bmatrix}
      -1 & 2 & -2\\
      0 & 2 & -5\\
      0 & 0 & -6,5
    \end{bmatrix}}_{U}
  \end{equation}
  Daí, iniciamos resolvendo o seguinte sistema triangular inferior $Ly = b$, i.e.
  \begin{align}
    &\hleq{y_1 = 6} \\
    &-3y_1 + y_2 = -11 \\
    &\qquad\Rightarrow \hleq{y_2 = 7}\\
    &-y_1 - 1,5y_2 + y_3 = -10 \\
    &\qquad\Rightarrow \hleq{y_3 = 6,5}.
  \end{align}
  Por fim, computamos a solução $x$ resolvendo o sistema triangular superior $Ux=y$, i.e.
  \begin{align}
    &-6,5x_3 = 6,5 \\
    &\qquad \Rightarrow \hleq{x_3 = -1},\\
    &2x_2 - 5x_3 = 7 \\
    &\qquad \Rightarrow \hleq{x_2 = 1}\\
    &-x_1 + 2x_2 - 2x_3 = 6 \\
    &\qquad \Rightarrow \hleq{x_1 = -2}.
  \end{align}

\begin{lstlisting}
import numpy as np

# mat coefs
A = np.array([[-1., 2., -2.],
              [3., -4., 1.],
              [1., -5., 3.]])
# vet termos const
b = np.array([6., -11., -10.])

# 1. LU
L, U = lu(A)

# 2. Ly = b
y = solSisTriaInf(L, b)

# 3. Ux = y
x = solSisTriaSup(U, y)
\end{lstlisting}
\end{ex}

\subsection{Fatoração LU com Pivotamento Parcial}

O algoritmo estudando acima não é aplicável no caso de o pivô ser nulo, o que pode ser corrigido através de permutações de linhas. \hl{Na fatoração LU com pivotamento parcial fazemos permutações de linha na matriz de forma que o pivô seja sempre aquele de maior valor em módulo}. Por exemplo, suponha que o elemento $a_{3,1}$ seja o maior valor em módulo na primeira coluna da matriz $A = U^{(0)}$ com
\begin{equation}
  U^{(0)} = \begin{bmatrix}
    u_{1,1}^{(0)} & u_{1,2}^{(0)} & u_{1,3}^{(0)} & \ldots & u_{1,n}^{(0)} \\
    u_{2,1}^{(0)} & u_{2,2}^{(0)} & u_{2,3}^{(0)} & \ldots & u_{2,n}^{(0)} \\
    \pmb{u_{3,1}^{(0)}} & u_{3,2}^{(0)} & u_{3,3}^{(0)} & \ldots & u_{3,n}^{(0)} \\
    \vdots & \vdots & \vdots & \ldots & \vdots \\
    u_{n,1}^{(0)} & u_{n,2}^{(0)} & a_{n,3}^{(0)} & \ldots & u_{n,n}^{(0)}
  \end{bmatrix}.
\end{equation}
Neste caso, o procedimento de eliminação na primeira coluna deve usar $u_{3,1}^{(0)}$ como pivô, o que requer a permutação entre as linhas $1$ e $3$ ($U_1^{(0)} \leftrightarrow U_3^{(0)}$). Isto pode ser feito utilizando-se da seguinte \emph{matriz de permutação}
\begin{equation}
  P =
  \begin{bmatrix}
    0 & 0 & 1 & \ldots & 0\\
    0 & 1 & 0 & \ldots & 0\\
    1 & 0 & 0 & \ldots & 0\\
    \vdots & \vdots & \vdots & \ldots & \vdots\\
    0 & 0 & 0 & \ldots & 1
  \end{bmatrix}.
\end{equation}
Com essa, iniciamos o procedimento de decomposição LU com $PA = L^{(0)}U^{(0)}$, onde $L^{(0)} = I_{n\times n}$ e $U^{(0)} = PA$. Caso sejam necessárias outras mudanças de linhas no decorrer do procedimento de decomposição, a matriz de permutação $P$ deve ser atualizada apropriadamente.

\begin{ex}\label{cap_sislin_sec_lu:ex:lup}
  Vamos fazer a decomposição LU com pivotamento parcial da seguinte matriz
  \begin{equation}
    A = \begin{bmatrix}
      -1 & 2 & -2\\
      3 & -4 & 1\\
      1 & -5 & 3
    \end{bmatrix}
  \end{equation}
  Começamos, tomando
  \begin{align}
    P^{(0)} =
    \begin{bmatrix}
      1 & 0 & 0\\
      0 & 1 & 0\\
      0 & 0 & 1
    \end{bmatrix},\\
    L^{(0)} =
    \begin{bmatrix}
      1 & 0 & 0\\
      0 & 1 & 0\\
      0 & 0 & 1      
    \end{bmatrix},\\
    U^{(0)} = 
    \begin{bmatrix}
      -1 & 2 & -2\\
      3 & -4 & 1\\
      1 & -5 & 3
    \end{bmatrix}
  \end{align}
  O candidato a pivô é o elemento $u^{(0)}_{2,1}$. Então, fazemos as permutações de linhas
  \begin{align}
    P_1^{((0)} &\leftrightarrow P_2^{(0)},\\
    U_1^{(0)} &\leftrightarrow U_2^{(0)}
  \end{align}
  e, na sequência, as operações elementares por linhas
  \begin{equation}
    U_{2:3}^{(1)}\leftarrow U_{2:3}^{(0)}-m^{(0)}_{2:3,1}U_1^{(0)},
  \end{equation}
  donde obtemos
  \begin{align}
    P^{(1)} &=
    \begin{bmatrix}
      0 & 1 & 0\\
      1 & 0 & 0\\
      0 & 0 & 1
    \end{bmatrix},\\
    L^{(1)} &=
    \begin{bmatrix}
      1 & 0 & 0\\
      -0,\overline{3} & 1 & 0\\
      0,\overline{3} & 0 & 1      
    \end{bmatrix},\\
    U^{(1)} &= 
    \begin{bmatrix}
      3 & -4 & 1\\
      0 & 0,\overline{6} & -1,\overline{6}\\
      0 & -3,\overline{6} & 2,\overline{6}
    \end{bmatrix}
  \end{align}
  Agora, o candidato a pivô é o elemento $u^{(1)}_{3,2}$. Assim, fazemos as permutações de linhas
  \begin{align}
    P^{(1)}_2 &\leftrightarrow P^{(1)}_3, \\
    U^{(1)}_2 &\leftrightarrow U^{(1)}_3
  \end{align}
  e análogo para os elementos da coluna 1 de $L$. Então, fazemos a operação elementar por linha
  \begin{equation}
    U^{(2)}_3 \leftarrow U^{(1)}_3 - m^{(1)}_{3,2}U^{(1)}_2
  \end{equation}
. Com isso, obtemos
  \begin{align}
    P^{(2)} &=
    \begin{bmatrix}
      0 & 1 & 0\\
      0 & 0 & 1\\
      1 & 0 & 0
    \end{bmatrix},\\
    L^{(2)} &=
    \begin{bmatrix}
      1 & 0 & 0\\
      0,\overline{3} & 1 & 0\\
      -0,\overline{3} & -0,\overline{18} & 1      
    \end{bmatrix},\\
    U^{(2)} &= 
    \begin{bmatrix}
      3 & -4 & 1\\
      0 & -3,\overline{6} & 2,\overline{6}\\
      0 & 0 & -1,\overline{18}
    \end{bmatrix}
  \end{align}
  Por fim, temos obtido a decomposição LU de $A$ na forma
  \begin{equation}
    PA = LU,
  \end{equation}
  com $P=P^{(2)}$, $L=L^{(2)}$ e $U=U^{(2)}$.

\begin{lstlisting}[caption=lup.py, label=cap_sislin_sec_lu:cod:lup]
import numpy as np

def lup(A):
    # num. de linhas
    n = A.shape[0]

    # inicialização
    U = A.copy()
    L = np.eye(n)
    P = np.eye(n)

    # decomposição
    for i in range(n-1):
        # permutação de linhas
        p = i + np.argmax(np.fabs(U[i:,i]))
        P[[i,p]] = P[[p,i]]
        U[[i,p]] = U[[p,i]]
        L[[i,p],:i] = L[[p,i],:i]
        # eliminação gaussiana
        for j in range(i+1,n):
            L[j,i] = U[j,i]/U[i,i]
            U[j,i:] -= L[j,i]*U[i,i:]

    return P, L, U

# matriz
A = np.array([[-1., 2, -2],
              [3, -4, 1],
              [1, -5, 3]])

P, L, U = lup(A)
\end{lstlisting}
\end{ex}

\begin{ex}
  Vamos computar a solução do seguinte sistema linear com o Método da Decomposição LU com Pivotamento Parcial.
  \begin{align}
    -x_1 + 2x_2 - 2x_3 &= 6\\
    3x_1 - 4x_2 + x_3 &= -11\\
    x_1 - 5x2 + 3x_3 &= -10.
  \end{align}
  No exemplo anterior (Exemplo \ref{cap_sislin_sec_lu:ex:lup}), vimos que a matriz de coeficientes $A$ deste sistema admite a seguinte decomposição LU
  \begin{equation}
    PA = LU
  \end{equation}
  com
  \begin{align}
    P &=
    \begin{bmatrix}
      0 & 1 & 0\\
      0 & 0 & 1\\
      1 & 0 & 0
    \end{bmatrix},\\
    L &=
    \begin{bmatrix}
      1 & 0 & 0\\
      0,\overline{3} & 1 & 0\\
      -0,\overline{3} & -0,\overline{18} & 1      
    \end{bmatrix},\\
    U &= 
    \begin{bmatrix}
      3 & -4 & 1\\
      0 & -3,\overline{6} & 2,\overline{6}\\
      0 & 0 & -1,\overline{18}
    \end{bmatrix}
  \end{align}
  Multiplicando o sistema a esquerda pela matriz $P$, obtemos
  \begin{gather}
    PA\pmb{x} = P\pmb{b}\\
    LU\pmb{x} = P\pmb{b}\\
    L(U\pmb{x}) = P\pmb{b}
  \end{gather}
  Com isso, resolvemos
  \begin{equation}
    L\pmb{y} = P\pmb{b}
  \end{equation}
  donde obtemos
  \begin{equation}
    \pmb{y} = (-11, -6,\overline{3}, 1,\overline{18})
  \end{equation}
  Então, resolvemos
  \begin{equation}
    U\pmb{x} = y
  \end{equation}
  donde obtemos a solução
  \begin{equation}
    \pmb{x} = (-2, 1, -1).
  \end{equation}
  
\begin{lstlisting}
# matriz
A = np.array([[-1., 2., -2.],
              [3., -4., 1.],
              [1., -5., 3.]])
b = np.array([6,-11,-10])

P, L, U = lup(A)

y = solSisTriaInf(L, P@b)
x = solSisTriaSup(U, y)
\end{lstlisting}
\end{ex}

\subsection*{Exercícios}

\begin{exer}
  Seja a matriz
  \begin{equation}
    A =
    \begin{bmatrix}
      -1 & 2 & -2\\
      3 & 4 & 1\\
      -4 & -5 & 3
    \end{bmatrix}
  \end{equation}
  \begin{enumerate}[a)]
  \item Compute sua decomposição LU sem pivotamento parcial.
  \item Compute sua decomposição LU com pivotamento parcial.
  \end{enumerate}
\end{exer}
\begin{resp}
  a)
  \begin{align}
    L &=
        \begin{bmatrix}
          1 & 0 & 0\\
          -3 & 1 & 0\\
          4 & 0 & 1
        \end{bmatrix}\\
    U &=
        \begin{bmatrix}
          -1 & 2 & 2\\
          0 & 10 & -5\\
          0 & 0 & 4.5
        \end{bmatrix}
  \end{align}
  b)
  \begin{align}
    P &=
        \begin{bmatrix}
          0 & 0 & 1\\
          1 & 0 & 0\\
          0 & 1 & 0
        \end{bmatrix}\\
    L &=
        \begin{bmatrix}
              0  & 0 & 0\\
           0,25  & 1 & 0\\
          -0,75  & 7,692\E-2 & 1
        \end{bmatrix}\\
    U &=
        \begin{bmatrix}
          -4 & -5 & 3\\
          0 & 3.25 & -2.75\\
          0 & 0 & 3.462
        \end{bmatrix}
  \end{align}  
\end{resp}

\begin{exer}\label{exer:lup_sol}
  Use o Método da Decomposição LU para resolver o sistema linear
  \begin{align}
    -x_1 + 2x_2 - 2x_3 &= -1\\
    3x_1 - 4x_2 + x_3 &= -4\\
    -4x_1 - 5x_2 + 3x_3 &= 20
  \end{align}
  usando  LU.
\end{exer}
\begin{resp}
  $x_1 = -3$, $x_2=-1$, $x_3 = 1$
\end{resp}

\begin{exer}
  Compute a decomposição LU da matriz
  \begin{equation}
    A =
    \begin{bmatrix}
      -1 & 0 & -2 & 1\\
      3 & 0 & 0 & -2\\
      1 & -1 & 0 & -1\\
      0 & 2 & -3 & 0
    \end{bmatrix}.
  \end{equation}
\end{exer}
\begin{resp}
  \begin{align}
    P &=
        \begin{bmatrix}
          0 & 1 & 0 & 0\\
          0 & 0 & 0 & 1\\
          1 & 0 & 0 & 0\\
          0 & 0 & 1 & 0
        \end{bmatrix}\\
    L &=
        \begin{bmatrix}
          1 & 0 & 0 & 0\\
          0 & 1 & 0 & 0\\
          -0.\overline{3} & 0 & 1 & 0\\
          0.\overline{3} & -0.5 & 0.75 & 1
        \end{bmatrix}\\
    U &=
        \begin{bmatrix}
          3 & 0 & 0  & -2\\
          0 & 2 & -3 & 0\\
          0 & 0 & -2 & 0.\overline{3}\\
          0 & 0 & 0 & -0.58\overline{3}
        \end{bmatrix}
  \end{align}
\end{resp}

\begin{exer}
  Use o Método da Decomposição LU para resolver o sistema linear
  \begin{align}
    -x_1 - 2x_3 + x_4 &= 1\\
    3x_1 - 2x_4 &= -7\\
    x_1 - x_2 - x_4 &= -3\\
    2x_2 - 3x_3 &= -3
  \end{align}
\end{exer}
\begin{resp}
  $x = (-1, 0, 1, 2)$
\end{resp}

\begin{exer}
  A matriz de Vandermonde{\vandermonde} é definida por
  \begin{equation}
    V(x_1,\ldots,x_n) =
    \begin{bmatrix}
      x_1^{n-1} & \cdots  & x_1^2 & x_1 & 1\\
      x_2^{n-1} & \cdots  & x_2^2 & x_2 & 1\\
      \vdots   &         & \vdots & \vdots & \vdots\\
      x_n^{n-1} & \cdots  & x_n^2 & x_n & 1\\
    \end{bmatrix}
  \end{equation}
  Compute a decomposição LU com pivotamento parcial das matrizes de Vandermonde.
  \begin{enumerate}[a)]
  \item $V(1,2,3)$\\
  \item $V(-2,-1,0,1)$\\
  \item $V(0,1, 0,25, 0,5, 0,75)$\\
  \item $V(-0,1, 0,5, 1, 2, 10)$
  \end{enumerate}
\end{exer}
\begin{resp}
  Dica: A matriz de Vandermonde pode ser alocada com o seguinte código:
\begin{lstlisting}
import numpy as np
x = np.array([1.,2,3])
n = x.size
V = np.ones((n,n))
for i in range(n-1):
    V[:,i] = x**(n-1-i)
print(V)
\end{lstlisting}
\end{resp}


\section{Norma e Número de Condicionamento}\label{cap_sislin_sec_ncond}

Nesta seção, fazemos uma rápida discussão sobre normas de vetores e matrizes e sobre o condicionamento de uma matriz.

\subsection{Norma $L^2$}

\subsubsection{Norma de Vetores}

A \hl{\emph{norma $L^2$}} de um dado vetor $\pmb{v} = (v_1, v_2, \dotsc, v_n) \in \mathbb{R}^n$ é \hl{definida por}
\begin{equation}\hleq
  \|\pmb{v}\| := \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2}.
\end{equation}

Lembrando que o \hl{\emph{produto interno}} de dois vetores $\pmb{u},\pmb{v}\in\mathbb{R}^2$ é \hl{definido por}
\begin{equation}\hleq
  \pmb{u}\cdot\pmb{v} := u_1v_1 + u_2v_2 + \cdots + u_nv_n,
\end{equation}
temos que
\begin{equation}\hleq
  \|\pmb{v}\| = \sqrt{\pmb{v}\cdot\pmb{v}}.
\end{equation}

\begin{ex}
  Sejam os vetores
  \begin{align}
    \pmb{u} &= (1, -2, 3, -4),\\
    \pmb{v} &= (-1, 2, 0, 1).
  \end{align}
  \begin{enumerate}[a)]
  \item
    \begin{align}
      \pmb{u}\cdot\pmb{v} &= u_1v_1 + u_2v_2 + \cdots + u_nv_n\\
                          &= 1\cdot(-1) + (-2)\cdot 2 + 3\cdot 0 + (-4)\cdot 1\\
                          &= -1 - 4 + 0 - 4\\
                          &= -9.
    \end{align}

\begin{lstlisting}
import numpy as np
u = np.array([1., -2., 3., -4.])
v = np.array([-1., 2., 0., 1.])
udv = np.dot(u,v)
print(f'\nu.v = {udv}')
\end{lstlisting}

  \item
    \begin{align}
      \|\pmb{v}\| &= \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2}\\
                  &= \sqrt{(-1)^2 + 2^2 + 0^2 + 1^2}\\
                  &= \sqrt{1 + 4 + 1}\\
                  &= \sqrt{6} = 2,4495\E+0.
    \end{align}

\begin{lstlisting}
import numpy as np
import numpy.linalg as npla
v = np.array([-1., 2., 0., 1.])
norm_v = npla.norm(v)
print(f'\n||v|| = {norm_v}')
\end{lstlisting}
  \end{enumerate}
\end{ex}


\begin{prop}\normalfont{\hl{(Propriedades da Norma.)}}
  Dados os vetores $\pmb{u},\pmb{v} \in \mathbb{R}^n$ e um escalar $\lambda\in\mathbb{R}$, temos:
  \begin{enumerate}[a)]
  \item \hl{\emph{Positividade}}
    \begin{align}
      &\hleq\|\pmb{v}\| \geq 0.\\
      &\hleq\|\pmb{v}\| = 0 \Leftrightarrow \pmb{v} = \pmb{0}.
    \end{align}
    
  \item \hl{\emph{Multiplicação por escalar}}
    \begin{equation}\hleq
      \|\lambda \pmb{v}\| = |\lambda|\cdot \|\pmb{v}\|.
    \end{equation}
    
  \item \hl{\emph{Desigualdade de Cauchy}}{\cauchy}\hl{\emph{-Schwarz}}{\schwarz}
    \begin{equation}\label{cap_sislin_sec_ncond:eq:cauchy-schwarz}
      \hleq \pmb{u}\cdot \pmb{v} \leq \|\pmb{u}\|\cdot\|\pmb{v}\|
    \end{equation}

  \item \hl{\emph{Desigualdade triangular}}
    \begin{equation}\label{cap_sislin_sec_ncond:eq:destria}
      \hleq\|\pmb{u}+\pmb{v}\| \leq \|\pmb{u}\| + \|\pmb{v}\|
    \end{equation}
  \end{enumerate}
\end{prop}
\begin{dem}
  Sejam dados $\lambda\in\mathbb{R}$ e $\pmb{u},\pmb{v}\in\mathbb{R}^n$.  
  \begin{enumerate}[a)]
  \item Positividade.

    Observa-se diretamente que $v_1^2+v_2^2+\cdots + v_n^2\geq 0$. Então, como a raiz quadrada é uma função não-negativa, concluímos que
    \begin{equation}
      \begin{aligned}
        \|\pmb{v}\| &:= \sqrt{v_1^2+v_2^2+\cdots + v_n^2}\\
        &\geq 0
      \end{aligned}
    \end{equation}
    No caso de $\pmb{v} = \pmb{0}$, temos $v_i=0$, $i=1,2,\dotsc,n$, donde
    \begin{equation}
      \begin{aligned}
        \|\pmb{v}\| &= \sqrt{v_1^2+v_2^2+\cdots + v_n^2}\\
        \|\pmb{0}\| &= \sqrt{0} = 0.
      \end{aligned}
    \end{equation}
    Ou seja, se $\pmb{v} = \pmb{0}$, então $\|\pmb{v}\| = 0$. Agora, se $\pmb{v}\neq \pmb{0}$, então tem-se $v_i\neq 0$ para algum $i=1, 2, \dotsc, n$. Logo, pela monotonicidade da função raiz quadrada, temos
      \begin{gather}
        v_1^2+v_2^2+\cdots + v_n^2 > 0\\
        \sqrt{v_1^2+v_2^2+\cdots + v_n^2} > \sqrt{0}\\
        \|\pmb{v}\| > 0.
      \end{gather}
      Portanto, concluímos que $\|\pmb{v}\| = 0$ se, e somente se, $\pmb{v} = \pmb{0}$.
      
    \item Multiplicação por escalar.
      
    Observamos que
    \begin{equation}
      \lambda\pmb{v} = (\lambda v_1, \lambda v_2, \dotsc, \lambda v_n).
    \end{equation}
    Então, segue por cálculo direto que
    \begin{align}
      \|\lambda\pmb{v}\| &= \sqrt{(\lambda v_1)^2 + (\lambda v_2)^2 + \cdots + (\lambda v_n)^2}\\
                         &= \sqrt{\lambda^2(v_1^2 + v_2^2 + \cdots + v_n^2)}\\
                         &= \sqrt{\lambda^2}\sqrt{v_1^2 + v_2^2 + \cdots + v_n^2}\\
                         &= |\lambda|\|\pmb{v}\|.
    \end{align}

  \item Desigualdade de Cauchy-Schwarz.

    Sem perda de generalidade, se $\pmb{v} = \pmb{0}$, então a desigualdade é imediatamente satisfeita. Suponhamos, agora, que $\pmb{v}\neq\pmb{0}$. Para qualquer $\alpha\in\mathbb{R}$, temos
    \begin{align}
      (\pmb{u}+\alpha\pmb{v})\cdot(\pmb{u}+\alpha\pmb{v}) &\geq 0\\
      \pmb{u}\cdot\pmb{u} + 2\alpha\pmb{u}\cdot\pmb{v} + \alpha^2\pmb{v}\cdot\pmb{v} &\geq 0\\
      \|\pmb{u}\|^2 + 2\alpha\pmb{u}\cdot\pmb{v} + \alpha^2\|\pmb{v}\|^2 &\geq 0
    \end{align}
    O lado direito desta desigualdade é um polinômio quadrático em $\alpha$. Para ser não-negativo para todo $\alpha$, seu discriminante precisa ser não-positivo, i.e.
    \begin{gather}
      (2\pmb{u}\cdot\pmb{v})^2 - 4\|\pmb{u}\|^2\|\pmb{v}\|^2 \leq 0\\
      (\pmb{u}\cdot\pmb{v})^2 \leq \|\pmb{u}\|^2\|\pmb{v}\|^2\\
      \sqrt{(\pmb{u}\cdot\pmb{v})^2} \leq \sqrt{\|\pmb{u}\|^2\|\pmb{v}\|^2}\\
      |\pmb{u}\cdot\pmb{v}| \leq \|\pmb{u}\|\|\pmb{v}\|
    \end{gather}
    Donde, concluímos que
    \begin{equation}
      \pmb{u}\cdot\pmb{v} \leq \|\pmb{u}\|\|\pmb{v}\|.
    \end{equation}

  \item Desigualdade triangular

    Consulte o Exercício \ref{cap_sislin_sec_ncond:exer:destria} para a demonstração da desigualdade triangular.
  \end{enumerate}
\end{dem}

\begin{exer}
  Vamos verificar a Desigualdade Triangular \eqref{cap_sislin_sec_ncond:eq:destria} para os vetores
  \begin{align}
    \pmb{u} &= (1, -2, 3, -4),\\
    \pmb{v} &= (-1, 2, 0, 1).    
  \end{align}
  De fato, temos
  \begin{align}
    \|\pmb{u}+\pmb{v}\| &= \sqrt{(u_1+v_1)^2 + (u_2+v_2)^2 + (u_3+v_3)^2 + (u_n+v_n)^2}\\
                        &= \sqrt{0^2 + 0^2 + 3^2 + 3^2}\\
                        &= \sqrt{18}\\
                        &= 4.2426\E+0
  \end{align}
  e
  \begin{align}
    \|\pmb{u}\|+\|\pmb{v}\| &= \sqrt{u_1^2 + u_2^2 + u_3^2 + u_4^2} \nonumber\\
                            &+ \sqrt{v_1^2 + v_2^2 + v_3^2 + v_4^2}\\
                            &= \sqrt{1^2 + (-2)^2 + 3^2 + (-4)^2} \nonumber\\
                            &+ \sqrt{(-1)^2 + 2^2 + 0^2 + 1^2}\\
                            &= \sqrt{30} + \sqrt{6}\\
                            &= 7,9267\E+0
  \end{align}
  Ou seja, temos que
  \begin{equation}
    \|\pmb{u}+\pmb{v}\| \leq \|\pmb{u}\|+\|\pmb{v}\| 
  \end{equation}
  como esperado.

\begin{lstlisting}
import numpy as np
u = np.array([1., -2., 3., -4.])
v = np.array([-1., 2., 0., 1.])
nupv = npla.norm(u+v)
nupnv = npla.norm(u) + npla.norm(v)
print('\nu.v <= ||u||.||v||?')
print(nupv <= nupnv)
\end{lstlisting}
\end{exer}

\subsubsection{Norma de Matrizes}

[[tag:revisar]]

A norma $L^2$ induzida de uma dada matriz real $A = [a_{ij}]_{i,j=1}^n$ é definida por
\begin{equation}
  \|A\| := \sup_{x\in\mathbb{R}^n, \|x\|=1} \|Ax\|.
\end{equation}
Pode-se mostrar que
\begin{equation}
  \|A\| = \sqrt{\lambda_{max}(A^TA)},
\end{equation}
onde $\lambda_{max}(A^TA) := \max\{|\lambda|;~\lambda\text{ é autovalor de }A^TA\}$.

\begin{prop}
  Dadas as matrizes reais $A, B$ $n\times n$, um vetor $v\in\mathbb{R}^2$ e um escalar $\lambda$, temos
  \begin{enumerate}[a)]
  \item $\|A\| \geq 0$.
  \item $\|A\|=0 \Leftrightarrow A=0$.
  \item $\|\lambda A\| = |\lambda|\cdot \|A\|$.
  \item $\|A+B\| \leq \|A\| + \|B\|$ (desigualdade triangular).
  \item $\|AB\| \leq \|A\|\|B\|$.
  \item $\|Av\| \leq \|A\|\|v\|$.
  \end{enumerate}
\end{prop}

\begin{ex}\label{ex:norma_matriz}
  A matriz
  \begin{equation}
    A =
    \begin{bmatrix}
      1 & -1 & 2\\
      -2 & \pi & 4\\
      7 & -5 & \sqrt{2}
    \end{bmatrix}
  \end{equation}
tem norma $L^2$
\begin{equation}
  \|A\| = 9,3909.
\end{equation}

% \ifisoctave
% No \verb+GNU Octave+, podemos fazer as computações acima com o seguinte \href{https://github.com/phkonzen/notas/blob/master/src/MatematicaNumerica/cap_sl_direto/dados/ex_norma_matriz/ex_norma_matriz.m}{código}:
% \verbatiminput{./cap_sl_direto/dados/ex_norma_matriz/ex_norma_matriz.m}
% \fi
\end{ex}

\subsection{Número de condicionamento}

\begin{flushleft}
  [[tag:revisar]]
\end{flushleft}

O número de condicionamento de uma matriz é uma medida referente a propagação de erros que ocorre da sua aplicação. Mais especificamente, assumamos que seja dada uma matriz invertível $A = [a_{ij}]_{i,j=1}^{n,n}$, um vetor $x\in\mathbb{R}^n$ e uma perturbação $\delta_x\in\mathbb{R}^n$. Além disso, sejam
\begin{align}
  y &= Ax\\
  y + \delta_y &= A(x+\delta_x).
\end{align}
Ou seja, $\delta_y$ é a perturbação em $y$ propagada da aplicação de $A$ em $x$ com perturbação $\delta_x$.

Agora, podemos estimar a razão entre os erros relativos $e_{rel}(y) := \|\delta_y\|/\|y\|$ e $e_{rel}(x) := \|\delta_x\|/\|x\|$ da seguinte forma 
\begin{align}
  \frac{\frac{\|y\|}{\|\delta_y\|}}{\frac{\|x\|}{\|\delta_x\|}} &= \frac{\|y\|}{\|\delta_y\|}\frac{\|\delta_x\|}{\|x\|}\\
  &=\frac{\|Ax\|}{\|\delta_y\|}\frac{\|A^{-1}\delta_y\|}{\|x\|} \\
  &\leq \frac{\|A\|\|x\|\|A^{-1}\|\|\delta_y\|}{\|\delta_y\|\|x\|}\\
  &\leq \|A\|\|A^{-1}\|.
\end{align}
Logo, temos a seguinte estimativa de propagação de erro
\begin{equation}
  e_{rel}(y) \leq \|A\|\|A^{-1}\|e_{rel}(x).
\end{equation}

Isto nos motiva a definir o \emph{número de condicionamento} da matriz $A$ por
\begin{equation}
  \kappa(A) := \|A\|\|A^{-1}\|.
\end{equation}

\begin{obs}
  A matriz identidade tem o menor número de condicionamento, o qual é
  \begin{equation}
    \kappa(I) = 1.
  \end{equation}
\end{obs}

\begin{ex}\label{ex:kappa}
  Um exemplo de uma matriz bem condicionada é
  \begin{equation}
    A =
    \begin{bmatrix}
      1 & -1 & 2\\
      -2 & \pi & 4\\
      7 & -5 & \sqrt{2}
    \end{bmatrix},
  \end{equation}
  cujo número de condicionamento é $\kappa(A) = 13,997$.

  Já, a matriz
  \begin{equation}
    B =
    \begin{bmatrix}
      1 & 0 & 0\\
      0 & 0 & -2\\
      10^{5} & 10^{-4} & 10^{5}
    \end{bmatrix},    
  \end{equation}
  tem número de condicionamento
  \begin{equation}
    \kappa(B) = 1,5811\times 10^{14},
  \end{equation}
  o que indica que $B$ é uma matriz mal condicionada.
\end{ex}

\subsection{Exercícios}

\begin{flushleft}
  [[tag:construcao]]
\end{flushleft}

\begin{exer}\label{exer:norma_numcond}
  Considere o seguinte sistema linear
  \begin{align}
    10^{-12}x_1 + 20x_2 + 3x_3 &= -1,\\
    2,001x_1 + 10^{-5}x_2 + - x_3 &= -2,\\
    x_1 - 2x_2 - 0,1x_3 &= 0,1.
  \end{align}
  \begin{enumerate}[a)]
  \item Compute a norma $L^2$ do vetor dos termos constantes deste sistema.
  \item Compute a norma $L^2$ da matriz dos coeficientes deste sistema.
  \item Compute o número de condicionamento da matriz dos coeficientes deste sistema.
  \end{enumerate}
\end{exer}
\begin{resp}
  % \ifisoctave 
  % \href{https://github.com/phkonzen/notas/blob/master/src/MatematicaNumerica/cap_sl_direto/dados/exer_norma_numcond/exer_norma_numcond.m}{Código.} 
  % \fi
  a) $2,2383$; b) $2,0323\E+1$; c) $3,5128\E+1$
\end{resp}

\subsubsection{Análise Numérica}

\begin{exer}\label{cap_sislin_sec_ncond:exer:destria}
  Mostre que a Desigualdade triangular
  \begin{equation}
    \|\pmb{u} + \pmb{v}\| \leq \|\pmb{u}\| + \|\pmb{v}\|
  \end{equation}
  vale para quaisquer $\pmb{u},\pmb{v}\in\mathbb{R}^n$.
\end{exer}
\begin{resp}
  Dica: use a Desigualdade de Cauchy-Schwarz \eqref{cap_sislin_sec_ncond:eq:cauchy-schwarz}.
\end{resp}


% \section{Método de eliminação gaussiana com pivotamento parcial com escala}\label{cap_sislin_sec_egauss_pivo}

% O método de eliminação gaussiana é suscetível a propagação dos erros de arredondamento, em particular, quando os pivôs são números próximos de zero. Isto pode ser mitigado com o chamado pivotamento parcial com escala. Nesta variação do método de eliminação gaussiana, o pivô é escolhido como sendo o candidato que é o maior em relação aos elementos em sua linha.

% Dado um sistema $Ax = b$ com $n$-equações e $n$-incógnitas, o método pode ser descrito pelo seguinte pseudo-código:
% \begin{enumerate}
%  \item $E \leftarrow [A ~ b]$.
%  \item Para $i=1, 2, \dotsc, n$, faça $s_i \leftarrow \max_{1\leq j \leq n}|e_{i,j}|$.
%  \item Para $i=1, 2, \dotsc, n-1$:
%    \begin{enumerate}[3.1]
%    \item Compute $j$ tal que
%      \begin{equation}
%        \frac{e_{j,i}}{s_j} \geq \frac{e_{k,i}}{s_k},\quad\forall k=i, i+1, \dotsc, n.
%      \end{equation}
%      \item Permute as linhas $i$ e $j$, i.e. $E_i \leftrightarrow E_j$.
%      \item Para $j=i+1, i+2, \dotsc, n$:
%        \begin{enumerate}[3.3.1]
%        \item $E_j \leftarrow E_j - \frac{e_{ji}}{e_{ii}}E_i$.
%        \end{enumerate}
%    \end{enumerate}
%  \item Para $i=n, n-1, \dotsc, 2$:   
%    \begin{enumerate}[4.1]
%      \item Para $j=i-1, i-2, \dotsc, 1$:
%        \begin{enumerate}[4.1.1]
%          \item $E_j \leftarrow E_j - \frac{e_{ji}}{e_{ii}}E_i$.
%        \end{enumerate}
%    \end{enumerate}
% \end{enumerate}

% \begin{ex}\label{ex:egauss_pivo}
%   Vamos empregar o método de eliminação gaussiana com pivotamento parcial com escala para resolvermos o seguinte sistema linear
%   \begin{align}
%     10^{-12}x_1 + 20x_2 + 3x_3 &= -1,\\
%     2,001x_1 + 10^{-5}x_2 - x_3 &= -2,\\
%     x_1 - 2x_2 - 0,1x_3 &= 0,1.
%   \end{align}
%   Para tanto, tomamos a matriz estendida
%   \begin{equation}
%     E =
%     \begin{bmatrix}
%       10^{-12} & 20 & 3 & -1\\
%       2,001 & 10^{-5} & -1 & -2\\
%       1 & -2 & -0,1 & 0,1
%     \end{bmatrix}
%   \end{equation}
%   e computamos os valores máximos em módulo dos elementos de cada linha da matriz $A$, i.e.
%   \begin{equation}
%     s = (20,~2,001,~2).
%   \end{equation}
%   Agora, observamos que $e_{2,1}$ é o maior pivô em escala, pois
%   \begin{equation}
%     \frac{e_{11}}{s_1} = 5\times 10^{-14}, \frac{e_{21}}{s_2} = 1, \frac{e_{31}}{s_3}=0,5.
%   \end{equation}
%   Então, fazemos a permutação entre as linhas $1$ e $2$, de forma a obtermos
%   \begin{equation}
%     E =
%     \begin{bmatrix}
%       2,001 & 10^{-5} & -1 & -2\\
%       10^{-12} & 20 & 3 & -1\\
%       1 & -2 & -0,1 & 0,1
%     \end{bmatrix}    
%   \end{equation}
%   Em seguida, eliminamos abaixo do pivô, e temos
%   \begin{equation}
%     E =
%     \begin{bmatrix}
%       2,001 & 10^{-5} & -1 & -2\\
%       0 & 20 & 3 & -1\\
%       0 & -2 & 3,9975\times 10^{-1} & 1,0995
%     \end{bmatrix}
%   \end{equation}
%   Daí, o novo pivô é escolhido como $e_{22}$, pois ambos candidatos tem o mesmo valor em escala
%   \begin{equation}
%     \frac{e_{2,2}}{s_2} = 1, \frac{e_{3,2}}{s_3} = 1.
%   \end{equation}
%   Logo, eliminamos abaixo do pivô para obtermos
%   \begin{equation}
%     E =
%     \begin{bmatrix}
%       2,001 & 10^{-5} & -1 & -2\\
%       0 & 20 & 3 & -1\\
%       0 & 0 & 6,9975\times 10^{-1} & 9,9950
%     \end{bmatrix}
%   \end{equation}
%   Procedendo a eliminação para cima, obtemos a matriz escalonada reduzida
%   \begin{equation}
%     E =
%     \begin{bmatrix}
%       1 & 0 & 0 & -2.8567\E-1\\
%       0 & 1 & 0 & -2.6425\E-1\\
%       0 & 0 & 1 & 1,4284\E+0
%     \end{bmatrix}
%   \end{equation}

% % \ifisoctave
% % No \verb+GNU Octave+, podemos fazer as computações acima com o seguinte \href{https://github.com/phkonzen/notas/blob/master/src/MatematicaNumerica/cap_sl_direto/dados/ex_egauss_pivo/ex_egauss_pivo.m}{código}:
% % \verbatiminput{./cap_sl_direto/dados/ex_egauss_pivo/ex_egauss_pivo.m}
% % \fi
% \end{ex}

% \subsection*{Exercícios}

% \begin{exer}\label{exer:egauss_pivo_exec}
%   Use o método de eliminação gaussiana com pivotamento parcial com escala para obter a matriz escalonada reduzida do seguinte sistema
%   \begin{align}
%     -2\times 10^{-12}x_1 + 10x_2 - 3\times 10^{-4}x_3 &= 2\\
%     10^5x_1 + 10^{-13}x_2 - x_3 &= -2\\
%     x_1 - 2x_2 + 3\times 10^{-7}x_3 &= 4
%   \end{align}
% \end{exer}
% \begin{resp}
%   % \ifisoctave 
%   % \href{https://github.com/phkonzen/notas/blob/master/src/MatematicaNumerica/cap_sl_direto/dados/exer_egauss_pivo_exec/exer_egauss_pivo_exec.m}{Código.} 
%   % \fi
%   $$
%   \begin{bmatrix}
%    1,0000 &  0,0000 &  0,0000 & 6,2588\E-1\\
%    0,0000 &  1,0000 &  0,0000 & -1,6777\E+0\\
%    0,0000 &  0,0000 &  1,0000 & 6,2589\E+4
%   \end{bmatrix}
%   $$
% \end{resp}

\section{Métodos de Jacobi e de Gauss-Seidel}\label{cap_sislin_sec_jgs}

\begin{flushleft}
  [[tag:revisar]]
\end{flushleft}

Nesta seção, discutiremos os métodos de Jacobi\footnote{Carl Gustav Jacob Jacobi, 1804 - 1851, matemático alemão. Fonte: \href{https://en.wikipedia.org/wiki/Carl_Gustav_Jacob_Jacobi}{Wikipedia}.} e de Gauss-Seidel\footnote{Johann Carl Friedrich Gauss, 1777 - 1855, matemático alemão. Philipp Ludwig von Seidel, 1821 - 1896, matemático alemão. Fonte: \href{https://en.wikipedia.org/wiki/Philipp_Ludwig_von_Seidel}{Wikipedia}.} para a aproximação da solução de sistemas lineares.

\subsection{Método de Jacobi}

\begin{flushleft}
  [[tag:revisar]]
\end{flushleft}

Dado um sistema $A\pmb{x} = \pmb{b}$ com $n$ equações e $n$ incógnitas, consideramos a seguinte decomposição da matriz $A = L + D + U$:
\begin{align}
  A &=
  \begin{bmatrix}
    a_{11} & a_{12} & a_{13} & \ldots & a_{1n}\\
    a_{21} & a_{22} & a_{23} & \ldots & a_{2n}\\
    a_{31} & a_{32} & a_{33} & \ldots & a_{3n}\\
    \vdots & \vdots & \vdots & \ldots & \vdots\\
    a_{n1} & a_{n2} & a_{n3} & \ldots & a_{nn}\\
  \end{bmatrix}\\
    &= \underbrace{\begin{bmatrix}
    0 & 0 & 0 & \ldots & 0\\
    a_{21} & 0 & 0 & \ldots & 0\\
    a_{31} & a_{32} & 0 & \ldots & 0\\
    \vdots & \vdots & \vdots & \ldots & \vdots\\
    a_{n1} & a_{n2} & a_{n3} & \ldots & 0\\
  \end{bmatrix}}_{L}\\
    &+ \underbrace{\begin{bmatrix}
    a_{11} & 0 & 0 & \ldots & 0\\
    0 & a_{22} & 0 & \ldots & 0\\
    0 & 0 & a_{33} & \ldots & 0\\
    \vdots & \vdots & \vdots & \ldots & \vdots\\
    0 & 0 & 0 & \ldots & a_{nn}\\
  \end{bmatrix}}_{D}\\
  &+ \underbrace{\begin{bmatrix}
    0 & a_{12} & a_{13} & \ldots & a_{1n}\\
    0 & 0 & a_{23} & \ldots & a_{2n}\\
    0 & 0 & a_{33} & \ldots & a_{3n}\\
    \vdots & \vdots & \vdots & \ldots & \vdots\\
    0 & 0 & 0 & \ldots & a_{nn}\\
  \end{bmatrix}}_{U}.
\end{align}
Isto é, a matriz $A$ decomposta como a soma de sua parte triangular inferior $L$, de sua diagonal $D$ e de sua parte triangular superior $U$.

Desta forma, podemos reescrever o sistema $A\pmb{x}=b$ da seguinte forma:
\begin{align}
  A\pmb{x} = \pmb{b} &\Leftrightarrow (L + D + U)\pmb{x} = \pmb{b}\\
         &\Leftrightarrow D\pmb{x} = -(L+U)\pmb{x} + \pmb{b}\\
         &\Leftrightarrow \pmb{x} = -D^{-1}(L+U)\pmb{x} + D^{-1}\pmb{b}.
\end{align}
Ou seja, resolver o sistema $A\pmb{x} = \pmb{b}$ é equivalente a resolver o problema de ponto fixo
\begin{equation}
  \pmb{x} = T_J\pmb{x} + \pmb{c}_J,
\end{equation}
onde $T_J = -D^{-1}(L+U)$ é chamada de \emph{matriz de Jacobi}\index{matriz de!Jacobi} e $\pmb{c}_J = D^{-1}\pmb{b}$ é chamado de \emph{vetor de Jacobi}\index{vetor de!Jacobi}.

\begin{ex}\label{ex:jacobi_intro}
  Consideremos o sistema linear $A\pmb{x} = \pmb{b}$ com
  \begin{equation}
    A =
    \begin{bmatrix}
      -4 & 2 & -1 \\
      -2 & 5 & 2 \\
       1 & -1 & -3
    \end{bmatrix},\quad
    \pmb{b} =
    \begin{bmatrix}
      -11\\ -7\\ 0
    \end{bmatrix}.
  \end{equation}
  Este sistema tem solução $\pmb{x} = (2, -1, 1)$. Neste caso, temos a decomposição $A = L + D + U$ com
  \begin{equation}
    L = \begin{bmatrix} 
      0 & 0 & 0 \\
      -2 & 0 & 0 \\
       1 & -1 & 0
     \end{bmatrix},\quad
    D = \begin{bmatrix}
      -4 & 0 & 0 \\
      0 & 5 & 0 \\
       0 & 0 & -3                  
     \end{bmatrix}
  \end{equation}
  e
  \begin{equation}
    U = \begin{bmatrix}
      0 & 2 & -1 \\
      0 & 0 & 2 \\
       0 & 0 & 0
    \end{bmatrix}.
  \end{equation}
  Ainda, observamos que
  \begin{align}
    T_J\pmb{x} + \pmb{c}_J &= -D^{-1}(L+U)\pmb{x} + D^{-1}\pmb{b}\\
    &= \underbrace{\begin{bmatrix}
        0 & 1/2 & 1/4 \\
        2/5 & 0 & -2/5 \\
        1/3 & -1/3 & 0
      \end{bmatrix}}_{T_J}
      \underbrace{\begin{bmatrix}
        2 \\
        -1 \\
        1             
      \end{bmatrix}}_{\pmb{x}} +
      \underbrace{\begin{bmatrix}
       11/4 \\
       -7/5 \\
       0
      \end{bmatrix}}_{\pmb{c}_J}\\
  &= \underbrace{\begin{bmatrix}
        2 \\
        -1 \\
        1             
      \end{bmatrix}}_{\pmb{x}}.
  \end{align}
% \ifisoctave
% No \verb+GNU Octave+, podemos fazer as computações acima com o seguinte \href{https://github.com/phkonzen/notas/blob/master/src/MatematicaNumerica/cap_sl_iter/dados/ex_jacobi_intro/ex_jacobi_intro.m}{código}:
% \verbatiminput{./cap_sl_iter/dados/ex_jacobi_intro/ex_jacobi_intro.m}
% \fi
\end{ex}

Com o exposto acima, o \emph{método de Jacobi} consiste na seguinte iteração de ponto fixo
\begin{align}
  \pmb{x}^{(1)} &= \text{aprox. inicial},\\
  \pmb{x}^{(k+1)} &= T_J\pmb{x}^{(k)} + \pmb{c}_J,\label{eq:iter_jacobi_mat}
\end{align}
onde $\pmb{x}^{(k)} = (x_1^{(k)}, x_2^{(k)}, \dotsc, x_n^{(k)})$ é a $k$-ésima aproximação (ou iterada) de Jacobi.

A iteração \eqref{eq:iter_jacobi_mat} pode ser equivalentemente escrita na seguinte forma algébrica
\begin{equation}
  x_i^{(k+1)} = \frac{{\displaystyle b_i - \sum_{\overset{j=1}{j\neq i}}^n a_{ij}x_j^{(k)}}}{a_{ii}},~i=1, 2, \dotsc, n,
\end{equation}
a qual não requer a computação da matriz $T_J$ e $\pmb{c}_J$.

\begin{ex}\label{ex:jacobi_exec}
  Consideremos o sistema $A\pmb{x} = \pmb{b}$ com
  \begin{equation}
    A =
    \begin{bmatrix}
      -4 & 2 & -1 \\
      -2 & 5 & 2 \\
       1 & -1 & -3
    \end{bmatrix},\quad
    \pmb{b} =
    \begin{bmatrix}
      -11\\ -7\\ 0
    \end{bmatrix}.
  \end{equation}
  Aplicando o método de Jacobi com aproximação inicial $\pmb{x}^{(1)} = (0, 0, 0)$ obtemos os resultados da Tabela \ref{tab:ex_jacobi_exec}.

  \begin{table}[h!]
    \centering
    \begin{tabular}{l|cc}
      k & $\pmb{x}^{(k)}$ & $\|A\pmb{x}^{(k)}-\pmb{b}\|$\\\hline
      1 & $(0,0,~0,0,~0,0)$ & $1,3\E+1$\\
      2 & $(2,8,~-1,4,~0,0)$ & $7,4\E+0$ \\
      3 & $(2,0,~-0,3,~1,4)$ & $4,6\E+0$ \\
      4 & $(2,3,~-1,1,~0,8)$ & $2,2\E+0$ \\
      5 & $(2,0,~-0,8,~1,1)$ & $1,4\E+0$ \\
      6 & $(2,1,~-1,1,~0,9)$ & $6,9\E-1$ \\
      7 & $(2,0,~-0,9,~1,0)$ & $4,2\E-1$ \\
      8 & $(2,0,~-1,0,~1,0)$ & $2,2\E-1$ \\
      9 & $(2,0,~-1,0,~1,0)$ & $1,3\E-1$ \\
      10 & $(2,0,~-1,0,~1,0)$ & $6,9\E-2$ \\\hline
    \end{tabular}
    \caption{Resultados referentes ao Exemplo \ref{ex:jacobi_exec}.}
    \label{tab:ex_jacobi_exec}
  \end{table}

% \ifisoctave
% No \verb+GNU Octave+, podemos obter os resultados reportados na Tabela \ref{tab:ex_jacobi_exec} com o seguinte \href{https://github.com/phkonzen/notas/blob/master/src/MatematicaNumerica/cap_sl_iter/dados/ex_jacobi_exec/ex_jacobi_exec.m}{código}:
% \verbatiminput{./cap_sl_iter/dados/ex_jacobi_exec/ex_jacobi_exec.m}
% \fi
\end{ex}

\subsection{Método de Gauss-Seidel}

\begin{flushleft}
  [[tag:revisar]]
\end{flushleft}

Como acima, começamos considerando um sistema linear $A\pmb{x} = \pmb{b}$ e a decomposição $A = L + D + U$, onde $L$ é a parte triangular inferior de $A$, $D$ é sua parte diagonal e $U$ sua parte triangular superior. Então, observamos que
\begin{align}
  A\pmb{x} = \pmb{b} &\Leftrightarrow (L + D + U)\pmb{x} = \pmb{b}\\
  &\Leftrightarrow (L+D)\pmb{x} = -U\pmb{x} + \pmb{b}\\
  &\Leftrightarrow \pmb{x} = -(L+D)^{-1}U\pmb{x} + (L+D)^{-1}\pmb{b}.
\end{align}
Isto nos leva a iteração de Gauss-Seidel
\begin{align}
  \pmb{x}^{(1)} = \text{aprox. inicial},\\
  \pmb{x}^{(k+1)} = T_G\pmb{x}^{(k)} + \pmb{c}_G,\label{eq:iter_gs_mat}
\end{align}
onde $T_G = -(L+D)^{-1}U$ é a chamada \emph{matriz de Gauss-Seidel}\index{matriz de!Gauss-Seidel} e $\pmb{c}_G = (L+D)^{-1}\pmb{b}$ é o chamado \emph{vetor de Gauss-Seidel}\index{vetor de!Gauss-Seidel}.

Observamos, também, que a iteração \eqref{eq:iter_gs_mat} pode ser reescrita na seguinte forma algébrica
\begin{equation}
  x_i^{(k+1)} = \frac{{\displaystyle b_i - \sum_{j=1}^{i-1} a_{ij}x_j^{(k+1)} - \sum_{j=i+1}^{n} a_{ij}x_j^{(k)}}}{a_{ii}},~i=1, 2, \dotsc, n.
\end{equation}

\begin{ex}\label{ex:gs_exec}
  Consideremos o sistema $A\pmb{x} = \pmb{b}$ com
  \begin{equation}
    A =
    \begin{bmatrix}
      -4 & 2 & -1 \\
      -2 & 5 & 2 \\
       1 & -1 & -3
    \end{bmatrix},\quad
    \pmb{b} =
    \begin{bmatrix}
      -11\\ -7\\ 0
    \end{bmatrix}.
  \end{equation}
  Aplicando o método de Gauss-Seidel com aproximação inicial $\pmb{x}^{(1)} = (0, 0, 0)$ obtemos os resultados da Tabela \ref{tab:ex_gs_exec}.

  \begin{table}[h!]
    \centering
    \begin{tabular}{l|cc}
      k & $\pmb{x}^{(k)}$ & $\|A\pmb{x}^{(k)}-\pmb{b}\|$\\\hline
      1 & $(0,0,~0,0,~0,0)$ & $1,3\E+1$ \\
      2 & $(2,8,~-0,3,~1,0)$ & $2,6\E+0$ \\
      3 & $(2,3,~-0,9,~1,1)$ & $1,2\E+0$ \\
      4 & $(2,0,~-1,0,~1,0)$ & $2,5\E-1$ \\
      5 & $(2,0,~-1,0,~1,0)$ & $4,0\E-2$ \\\hline
    \end{tabular}
    \caption{Resultados referentes ao Exemplo \ref{ex:gs_exec}.}
    \label{tab:ex_gs_exec}
  \end{table}

% \ifisoctave
% No \verb+GNU Octave+, podemos obter os resultados reportados na Tabela \ref{tab:ex_gs_exec} com o seguinte \href{https://github.com/phkonzen/notas/blob/master/src/MatematicaNumerica/cap_sl_iter/dados/ex_gs_exec/ex_gs_exec.m}{código}:
% \verbatiminput{./cap_sl_iter/dados/ex_gs_exec/ex_gs_exec.m}
% \fi
\end{ex}

\subsection{Análise de convergência}

\begin{flushleft}
  [[tag:revisar]]
\end{flushleft}

Observamos que ambos os métodos de Jacobi e de Gauss-Seidel consistem de iterações da forma
\begin{equation}
  \pmb{x}^{(k+1)} = T\pmb{x}^{(k)} + \pmb{c},~k=1, 2, \ldots,\label{eq:jgs_iter}
\end{equation}
com $x^{(1)}$ uma aproximação inicial dada, $T$ e $c$ a matriz e o vetor de iteração, respectivamente. O seguinte teorema nos fornece uma condição suficiente e necessária para a convergência de tais métodos.

\begin{teo}
  Para qualquer $\pmb{x}^{(1)}\in\mathbb{R}^n$, temos que a sequência $\{\pmb{x}^{(k+1)}\}_{k=1}^{\infty}$ dada por
  \begin{equation}
    \pmb{x}^{(k+1)} = T\pmb{x}^{(k)} + \pmb{c},
  \end{equation}
  converge para a solução única de $\pmb{x} = T\pmb{x} + \pmb{c}$ se, e somente se, $\rho(T) < 1$\footnote{$\rho(T)$ é o raio espectral da matriz $T$, i.e. o máximo dos módulos dos autovalores de $T$.}.
\end{teo}
\begin{dem}
  Veja \cite[Cap. 7, Sec. 7.3]{Burden2015a}.
\end{dem}

\begin{obs}(\normalfont{Taxa de convergência})
  Para uma iteração da forma \eqref{eq:jgs_iter}, vale
  \begin{equation}
    \|\pmb{x}^{(k)}-\pmb{x}\| \approx \rho(T)^{k-1}\|\pmb{x}^{(1)}-\pmb{x}\|,
  \end{equation}
onde $\pmb{x}$ é a solução de $\pmb{x} = T\pmb{x} + \pmb{c}$.
\end{obs}

\begin{ex}\label{ex:jacobi_exec}
  Consideremos o sistema $A\pmb{x} = \pmb{b}$ com
  \begin{equation}
    A =
    \begin{bmatrix}
      -4 & 2 & -1 \\
      -2 & 5 & 2 \\
       1 & -1 & -3
    \end{bmatrix},\quad
    \pmb{b} =
    \begin{bmatrix}
      -11\\ -7\\ 0
    \end{bmatrix}.
  \end{equation}
  Nos Exemplos \ref{ex:jacobi_exec} e \ref{ex:gs_exec} vimos que ambos os métodos de Jacobi e de Gauss-Seidel eram convergentes, sendo que este convergiu aproximadamente duas vezes mais rápido que esse. Isto é confirmado pelos raios espectrais das respectivas matrizes de iteração
  \begin{equation}
    \rho(T_J) \approx 0,56,\quad\rho(T_G) \approx 0,26.
  \end{equation}

% \ifisoctave
% No \verb+GNU Octave+, podemos obter os raios espectrais das matrizes de iteração de Jacobi e Gauss-Seidel com o seguinte \href{https://github.com/phkonzen/notas/blob/master/src/MatematicaNumerica/cap_sl_iter/dados/ex_jgs_conv/ex_jgs_conv.m}{código}:
% \verbatiminput{./cap_sl_iter/dados/ex_jgs_conv/ex_jgs_conv.m}
% \fi
\end{ex}

\begin{obs}{\normalfont{Matriz estritamente diagonal dominante}}
  Pode-se mostrar que se $A$ é uma matriz estritamente diagonal dominante, i.e. se
  \begin{equation}
    |a_{ii}| > \sum_{\overset{j=1}{j\neq i}}^n |a_{ij}|,~\forall i=1, 2, \ldots, n,
  \end{equation}
então ambos os métodos de Jacobi e de Gauss-Seidel são convergentes.
\end{obs}

\subsection*{Exercícios}

\begin{flushleft}
  [[tag:revisar]]
\end{flushleft}

\begin{exer}\label{exer:jacobi_exec}
  Considere o seguinte sistema linear
  \begin{align}
    -4x_1 + x_2 + x_3 - x_4 &= -1\\
    5x_2 -x_3 + 2x_4 &= 3\\
    -x_1 + 4x_3 - 2x_4 &= -2\\
    x_1 -x_2 -5x_4 &= 1
  \end{align}
  Compute a quinta iterada $x^{(5)}$ do método de Jacobi aplicado a este sistema com aproximação inicial $x^{(1)} = (1, 1, -1, -1)$. Também, compute $\|Ax^{(5)} - b\|$.
\end{exer}
\begin{resp}
  %   \ifisoctave 
  %   \href{https://github.com/phkonzen/notas/blob/master/src/MatematicaNumerica/cap_sl_iter/dados/exer_jacobi_exec/exer_jacobi_exec.m}{Código.} 
  % \fi
  $x^{(5)} = (-1,00256,~2,95365,~-1,95347,~0,97913)$; $\|Ax^{(5)}-b\| = 0,42244$
\end{resp}

\begin{exer}\label{exer:gs_exec}
  Considere o seguinte sistema linear
  \begin{align}
    -4x_1 + x_2 + x_3 - x_4 &= -1\\
    5x_2 -x_3 + 2x_4 &= 3\\
    -x_1 + 4x_3 - 2x_4 &= -2\\
    x_1 -x_2 -5x_4 &= 1
  \end{align}
  Compute a quinta iterada $x^{(5)}$ do método de Gauss-Seidel aplicado a este sistema com aproximação inicial $x^{(1)} = (1, 1, -1, -1)$. Também, compute $\|Ax^{(5)} - b\|$.
\end{exer}
\begin{resp}
  %   \ifisoctave 
  %   \href{https://github.com/phkonzen/notas/blob/master/src/MatematicaNumerica/cap_sl_iter/dados/exer_gs_exec/exer_gs_exec.m}{Código.} 
  % \fi
  $x^{(5)} = (-1,00423,~3,00316,~-2,00401,~0,99852)$; $\|Ax^{(5)}-b\| = 0,025883$
\end{resp}

\section{Método do gradiente}\label{cap_sislin_sec_metg}

\begin{flushleft}
  [[tag:revisar]]
\end{flushleft}

Começamos observando que se $A$ é uma matriz $n\times n$ positiva definida\footnote{$A$ é simétrica e $x^TAx > 0$ para todo $x\neq 0$.}, temos que $\pmb{x}\in\mathbb{R}^n$ é solução de
\begin{equation}\label{eq:metg_sislin}
  A\pmb{x} = \pmb{b}
\end{equation}
se, e somente se, $\pmb{x}$ é solução do seguinte problema de minimização
\begin{equation}\label{eq:metg_minprob}
  \min_{\pmb{x}\in\mathbb{R}^n}f(\pmb{x}) := \frac{1}{2}\pmb{x}^TA\pmb{x}-\pmb{x}^T\pmb{b}.
\end{equation}

O método do gradiente é um algoritmo da forma: dada uma aproximação inicial $\pmb{x}^{(1)}$ da solução de \eqref{eq:metg_minprob} (ou, equivalentemente, de \eqref{eq:metg_sislin}), computamos novas aproximações da forma iterativa
\begin{equation}
  \pmb{x}^{(k+1)} = \pmb{x}^{(k)} + \alpha^{(k)}\pmb{d}^{(k)},\quad k=1, 2, \ldots,
\end{equation}
onde $\alpha^{(k)}$ é o tamanho do passo (um escalar) e $\pmb{d}^{(k)}\in\mathbb{R}^n$ é a direção de busca.

Para escolhermos a direção $\pmb{d}^{(k)}$, tomamos a fórmula de Taylor de $f$ em torno da aproximação $\pmb{x}^{(k)}$
\begin{equation}\label{eq:metg_taylor}
  f(\pmb{x}^{(k+1)}) = f(\pmb{x}^{(k)}) + \alpha^{(k)}\nabla f(\pmb{x}^{(k)})\cdot \pmb{d}^{(k)} + O\left((\alpha^{(k)})^2\right),
\end{equation}
com $\alpha^{(k)}\to 0$, onde $\nabla f$ denota o gradiente de $f$, i.e.
\begin{align}
  \nabla f(\pmb{x}^{(k)}) &= \left(\frac{\p f}{\p x_1}(\pmb{x}^{(k)}), \frac{\p f}{\p x_2}(\pmb{x}^{(k)}), \dotsc, \frac{\p f}{\p x_n}(\pmb{x}^{(k)})\right)\\
  &= A\pmb{x}^{(k)}-\pmb{b}.
\end{align}


De \eqref{eq:metg_taylor}, segue que se
\begin{equation}
  \nabla f(\pmb{x}^{(k)})\cdot \pmb{d}^{(k)} < 0,
\end{equation}
então $f(\pmb{x}^{(k+1)}) < f(\pmb{x}^{(k)})$ se $\alpha^{(k)}$ é suficientemente pequeno. Em particular, podemos escolher
\begin{equation}
  \pmb{d}^{(k)} = -\nabla f(\pmb{x}^{(k)}),
\end{equation}
se $\nabla f(\pmb{x}^{(k)})\neq 0$.

Do exposto acima, temos a \pmb{iteração do método do gradiente}
\begin{align}
  \pmb{x}^{(1)} &= \text{aprox. inicial}\\
  \pmb{x}^{(k+1)} &= \pmb{x}^{(k)} - \alpha^{(k)}\pmb{r}^{(k)},~k=1, 2, \ldots,
\end{align}
onde $\pmb{r}^{(k)}$ é o resíduo da iterada $k$ dado por
\begin{equation}
  \pmb{r}^{(k)} = A\pmb{x^{(k)}}-\pmb{b}.
\end{equation}

\begin{ex}\label{ex:metg_pc}
  Consideremos o sistema $Ax = b$ com
  \begin{equation}
    A =
    \begin{bmatrix}
      2 & -1 & 0 & 0\\
      -1 & 2 & -1 & 0\\
      0 & -1 & 2 & -1 \\
      0 & 0 & -1 & 2
    \end{bmatrix},\quad
    b =
    \begin{bmatrix}
      -3\\
      2\\
      2\\
      -3
    \end{bmatrix}.
  \end{equation}
  Na Tabela \ref{tab:metg_pc} temos os resultados do emprego do método do gradiente com $\pmb{x}^{(1)} = (0, 0, 0, 0)$ e com passo constante $\alpha^{(k)}\equiv 0,5$.

  \begin{table}[h!]
    \centering
    \caption{Resultados referentes ao Exemplo \ref{ex:metg_pc}.}
    \label{tab:metg_pc}
    \begin{tabular}{l|c|c}
      k & $\pmb{x}^{(k)}$ & $\|A\pmb{x}^{(k)}-\pmb{b}\|$\\\hline
      1 & $(0,0,~0,0,~0,0,~0,0)$ & $5,1\E+0$\\
      2 & $(-1,5,~1,0,~1,0,~-1,5)$ & $1,6\E+0$\\
      3 & $(-1,0,~0,8,~0,8,~-1,0)$ & $5,0\E-1$\\
      4 & $(-1,1,~0,9,~0,9,~-1,1)$ & $1,8\E-1$\\
      5 & $(-1,1,~0,9,~0,9,~-1,1)$ & $8,8\E-2$\\
      6 & $(-1,1,~0,9,~0,9,~-1,1)$ & $6,2\E-2$\\
      7 & $(-1,0,~0,9,~0,9,~-1,0)$ & $4,9\E-2$\\
      8 & $(-1,0,~0,9,~0,9,~-1,0)$ & $4,0\E-2$\\
      9 & $(-1,0,~0,9,~0,9,~-1,0)$ & $3,2\E-2$\\
      10 & $(-1,0,~1,0,~1,0,~-1,0)$ & $2,6\E-2$\\
      11 & $(-1,0,~1,0,~1,0,~-1,0)$ & $2,1\E-2$\\\hline
    \end{tabular}
  \end{table}

% \ifisoctave
% No \verb+GNU Octave+, podemos fazer as computações acima com o seguinte \href{https://github.com/phkonzen/notas/blob/master/src/MatematicaNumerica/cap_sl_iter/dados/ex_metg_pc/ex_metg_pc.m}{código}:
% \verbatiminput{./cap_sl_iter/dados/ex_metg_pc/ex_metg_pc.m}
% \fi
\end{ex}

\subsection{Escolha do passo}

\begin{flushleft}
  [[tag:revisar]]
\end{flushleft}

Da iteração do método do gradiente, temos que a melhor escolha do passo $\alpha^{(k)}$ é tal que
\begin{equation}
  f(\pmb{x}^{(k)}+\alpha^{(k)}\pmb{r}^{(k)}) = \min_{\alpha > 0} f(\pmb{x}^{(k)}+\alpha\pmb{r}^{(k)}).
\end{equation}
Desta forma,
\begin{align}
  \frac{\dd}{\dd \alpha}f(\pmb{x}^{(k)}+\alpha \pmb{r}^{(k)}) = 0 &\Rightarrow \nabla f(\pmb{x}^{(k+1)})\cdot \pmb{r}^{(k)} = 0,\\
  &\Rightarrow \left(A(\pmb{x}^{(k)}+\alpha^{(k)}\pmb{r}^{(k)})-b\right)\cdot\pmb{r}^{(k)} = 0,\\
  &\Rightarrow (A\pmb{x}^{(k)}-\pmb{b})\cdot\pmb{r}^{(k)}+\alpha^{(k)}\pmb{r}^{(k)}\cdot A\pmb{r}^{(k)} = 0,
\end{align}
donde
\begin{equation}
  \alpha^{(k)} = - \frac{\pmb{r}^{(k)}\cdot\pmb{r}^{(k)}}{\pmb{r}^{(k)}\cdot A\pmb{r}^{(k)}}.
\end{equation}

\begin{ex}\label{ex:metg_alpha}
  Consideremos o sistema $Ax = b$ com
  \begin{equation}
    A =
    \begin{bmatrix}
      2 & -1 & 0 & 0\\
      -1 & 2 & -1 & 0\\
      0 & -1 & 2 & -1 \\
      0 & 0 & -1 & 2
    \end{bmatrix},\quad
    b =
    \begin{bmatrix}
      -3\\
      2\\
      2\\
      -3
    \end{bmatrix}.
  \end{equation}
  Na Tabela \ref{tab:metg_alpha} temos os resultados do emprego do método do gradiente com $\pmb{x}^{(1)} = (0, 0, 0, 0)$ e com passo
  \begin{equation}
    \alpha^{(k)} = - \frac{\pmb{r}^{(k)}\cdot\pmb{r}^{(k)}}{\pmb{r}^{(k)}\cdot A\pmb{r}^{(k)}}.
\end{equation}

  \begin{table}[h!]
    \centering
    \caption{Resultados referentes ao Exemplo \ref{ex:metg_alpha}.}
    \label{tab:metg_alpha}
    \begin{tabular}{l|c|c}
      k & $\pmb{x}^{(k)}$ & $\|A\pmb{x}^{(k)}-\pmb{b}\|$\\\hline
      1 & $(0,0,~0,0,~0,0,~0,0)$ & $5,1\E+0$\\
      2 & $(-1,1,~0,8,~0,8,~-1,1)$ & $1,5\E-1$\\
      3 & $(-1,0,~1,0,~1,0,~-1,0)$ & $3,0\E-2$\\
      4 & $(-1,0,~1,0,~1,0,~-1,0)$ & $8,8\E-4$\\
      5 & $(-1,0,~1,0,~1,0,~-1,0)$ & $1,8\E-4$\\\hline
    \end{tabular}
  \end{table}

% \ifisoctave
% No \verb+GNU Octave+, podemos fazer as computações acima com o seguinte \href{https://github.com/phkonzen/notas/blob/master/src/MatematicaNumerica/cap_sl_iter/dados/ex_metg_alpha/ex_metg_alpha.m}{código}:
% \verbatiminput{./cap_sl_iter/dados/ex_metg_alpha/ex_metg_alpha.m}
% \fi
\end{ex}

\subsection*{Exercícios}

\begin{flushleft}
  [[tag:construcao]]
\end{flushleft}

\section{Método do gradiente conjugado}\label{cap_sislin_sec_metgc}

\begin{flushleft}
  [[tag:revisar]]
\end{flushleft}

O método do gradiente conjugado é uma variação do método do gradiente (veja Seção \ref{cap_sislin_sec_metg}). Aqui, a solução de um dado sistema $A\pmb{x}=\pmb{b}$, com $A$ uma matriz positiva definida, é computada de forma iterativa por
\begin{align}
  \pmb{x}^{(1)} &= \text{aprox. inicial},\\
  \pmb{d}^{(1)} &= \pmb{r}^{(1)},\\
  &\\
  \pmb{x}^{(k+1)} &= \pmb{x}^{(k)} + \alpha_k\pmb{d}^{(k)},\\
  \alpha^{(k)} &= -\frac{\pmb{r}^{(k)}\cdot \pmb{d}^{(k)}}{\pmb{d}^{(k)}\cdot A\pmb{d}^{(k)}},\\
  \pmb{d}^{(k+1)} &= -\pmb{r}^{(k+1)}+\beta_k\pmb{d}^{(k)},\\
  \beta^{(k)} &= \frac{\pmb{r}^{(k+1)}\cdot A\pmb{d}^{(k)}}{\pmb{d}^{(k)}\cdot A\pmb{d}^{(k)}},
\end{align}
para $k = 1, 2, \ldots$, e $\pmb{r}^{(k)} = A\pmb{x}^{(k)}-\pmb{b}$.

\begin{ex}\label{ex:metgc_exec}
  Consideremos o sistema $Ax = b$ com
  \begin{equation}
    A =
    \begin{bmatrix}
      2 & -1 & 0 & 0\\
      -1 & 2 & -1 & 0\\
      0 & -1 & 2 & -1 \\
      0 & 0 & -1 & 2
    \end{bmatrix},\quad
    b =
    \begin{bmatrix}
      -3\\
      2\\
      2\\
      -3
    \end{bmatrix}.
  \end{equation}
  Na Tabela \ref{tab:metgc_exec} temos os resultados do emprego do método do gradiente conjugado com $\pmb{x}^{(1)} = (0, 0, 0, 0)$.

  \begin{table}[h!]
    \centering
    \caption{Resultados referentes ao Exemplo \ref{ex:metgc_exec}.}
    \label{tab:metgc_exec}
    \begin{tabular}{l|c|c}
      k & $\pmb{x}^{(k)}$ & $\|A\pmb{x}^{(k)}-\pmb{b}\|$\\\hline
      1 & $(0,~0,~0,~0)$ & $5.1\E+0$\\
      2 & $(-1,1,~0,8,~0,8,~-1,1)$ & $1,5\E-1$\\
      3 & $(-1,0,~1,0,~1,0,~-1,0)$ & $0,0\E+0$\\\hline
    \end{tabular}
  \end{table}

% \ifisoctave
% No \verb+GNU Octave+, podemos fazer as computações acima com o seguinte \href{https://github.com/phkonzen/notas/blob/master/src/MatematicaNumerica/cap_sl_iter/dados/ex_metgc_exec/ex_metgc_exec.m}{código}:
% \verbatiminput{./cap_sl_iter/dados/ex_metgc_exec/ex_metgc_exec.m}
% \fi
\end{ex}

\subsection*{Exercícios}

\begin{flushleft}
  [[tag:construcao]]
\end{flushleft}

% \section{Eliminação gaussiana}\label{cap_sislin_sec_egauss}

% Um sistema linear
% \begin{align}
%   a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n &= b_1 \label{eq:sl_fa_1}\\
%   a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n &= b_2\\
%   &\vdots \\
%   a_{n1}x_1 + a_{n2}x_2 + \cdots + a_{nn}x_n &= b_n.\label{eq:sl_fa_n}
% \end{align}
% pode ser escrito na forma matricial
% \begin{equation}
%   A\pmb{x} = \pmb{b},
% \end{equation}
% onde $A = [a_{ij}]_{i,j=1}^{n,n}$ é chamada de matriz dos coeficientes, $\pmb{x}=(x_1, x_2, \dotsc, x_n)$ é o vetor (coluna) das incógnitas e $\pmb{b}=(b_1, b_2, \dotsc, b_n)$ é o vetor (coluna) dos termos constantes.

% Outra forma matricial de representar o sistema \eqref{eq:sl_fa_1}-\eqref{eq:sl_fa_n} é pela chamada matriz estendida
% \begin{equation}
%   E = [A ~\pmb{b}].
% \end{equation}
% No caso, $E$ é a seguinte matriz $n \times (n+1)$
% \begin{equation}
%   E =
%   \begin{bmatrix}
%     a_{11} & a_{12} & \cdots & a_{1n} & b_1\\
%     a_{21} & a_{22} & \cdots & a_{2n} & b_2\\
%     \vdots & \vdots & \vdots & \vdots & \vdots\\
%     a_{n1} & a_{n2} & \cdots & a_{nn} & b_n
%   \end{bmatrix}
% \end{equation}

% O método de eliminação gaussiana consistem em realizar operações sobre as equações (sobre as linhas) do sistema \eqref{eq:sl_fa_1}-\eqref{eq:sl_fa_n} (da matriz estendida $E$) de forma a reescrevê-lo como um sistema triangular, ou diagonal. Para tanto, podemos utilizar as seguintes operações:
% \begin{enumerate}[1.]
% \item permutação entre as equações (linhas) ($E_i \leftrightarrow E_j$).
% \item multiplicação de uma equação (linha) por um escalar não nulo ($E_i \leftarrow \lambda E_i$).
% \item substituição de uma equação (linha) por ela somada com a multiplicação de uma outra por um escalar não nulo ($E_i \leftarrow E_i + \lambda E_j$).
% \end{enumerate}

% \begin{ex}\label{ex:egauss_exec}
%   O sistema linear
%   \begin{align}
%     -2x_1 - 3x_2 + 2x_3 + 3x_4 &= 10\label{eq:ex_egauss_exec_sl_1}\\
%     -4x_1 - 6x_2 + 6x_3 + 6x_4 &= 20\\
%     -2x_1 + 4x_3 + 6x_4 &= 10\\
%     4x_1 + 3x_2 - 8x_3 - 6x_4 &= -17\label{eq:ex_egauss_exec_sl_4}
%   \end{align}
% pode ser escrito na forma matricial $A\pmb{x}=\pmb{b}$, onde
% \begin{equation}
%   A =
%   \begin{bmatrix}
%     -2 & -3 & 2 & 3\\
%     -4 & -6 & 6 & 6\\
%     -2 & 0 & 4 & 6 \\
%     4 & 3 & -8 & -6
%   \end{bmatrix},
% \end{equation}
% $\pmb{x} = (x_1, x_2, x_3, x_4)$ e $\pmb{b} = (10, 20, 10, -17)$. Sua matriz estendida é
% \begin{equation}
%   E =
%   \begin{bmatrix}
%     -2 & -3 & 2 & 3 & 10\\
%     -4 & -6 & 6 & 6 & 20\\
%     -2 & 0 & 4 & 6 & 10\\
%     4 & 3 & -8 & -6 & -17
%   \end{bmatrix}
% \end{equation}
% Então, usando o método de eliminação gaussiana, temos
% \begin{align}
%   E &=
%   \begin{bmatrix}
%     -2 & -3 & 2 & 3 & 10\\
%     -4 & -6 & 6 & 6 & 20\\
%     -2 & 0 & 4 & 6 & 10\\
%     4 & 3 & -8 & -6 & -17
%   \end{bmatrix}
%   \begin{matrix}
%   \\
%   E_2\leftarrow E_2 - (e_{21}/\pmb{e_{11}})E_1\\
%   \\
%   \\
%   \end{matrix}\\
%   &\sim 
%   \begin{bmatrix}
%     \pmb{-2} & -3 & 2 & 3 & 10\\
%     0 & 0 & 2 & 0 & 0\\
%     -2 & 0 & 4 & 6 & 10\\
%     4 & 3 & -8 & -6 & -17
%   \end{bmatrix}
%   \begin{matrix}
%   \\
%   \\
%   E_3\leftarrow E_3 - (e_{31}/\pmb{e_{11}})E_1\\
%   \\
%   \end{matrix}\\
%   &\sim 
%   \begin{bmatrix}
%     \pmb{-2} & -3 & 2 & 3 & 10\\
%     0 & 0 & 2 & 0 & 0\\
%     0 & 3 & 2 & 3 & 0\\
%     4 & 3 & -8 & -6 & -17
%   \end{bmatrix}
%   \begin{matrix}
%   \\
%   \\
%   \\
%   E_4\leftarrow E_4 - (e_{41}/\pmb{e_{11}})E_1\\
%   \end{matrix}\\  
% &\sim 
%   \begin{bmatrix}
%     \pmb{-2} & -3 & 2 & 3 & 10\\
%     0 & 0 & 2 & 0 & 0\\
%     0 & \pmb{3} & 2 & 3 & 0\\
%     0 & -3 & -4 & 0 & 3
%   \end{bmatrix}
%   \begin{matrix}
%   \\
%   E_2 \leftrightarrow E_3\\
%   \\
%   \\
%   \end{matrix}\\
% &\sim 
%   \begin{bmatrix}
%     \pmb{-2} & -3 & 2 & 3 & 10\\
%     0 & \pmb{3} & 2 & 3 & 0\\
%     0 & 0 & 2 & 0 & 0\\
%     0 & -3 & -4 & 0 & 3
%   \end{bmatrix}
%   \begin{matrix}
%   \\
%   \\
%   \\
%   E_4 \leftarrow E_4 - (e_{42}/\pmb{e_{22}})E_2\\
%   \end{matrix}\\
% &\sim 
%   \begin{bmatrix}
%     \pmb{-2} & -3 & 2 & 3 & 10\\
%     0 & 3 & 2 & 3 & 0\\
%     0 & 0 & \pmb{2} & 0 & 0\\
%     0 & 0 & -2 & 3 & 3
%   \end{bmatrix}
%   \begin{matrix}
%   \\
%   \\
%   \\
%   E_4 \leftarrow E_4 - (e_{43}/\pmb{e_{33}})E_3\\
%   \end{matrix}\\
%     &\sim 
%       \begin{bmatrix}
%         \pmb{-2} & -3 & 2 & 3 & 10\\
%         0 & \pmb{3} & 2 & 3 & 0\\
%         0 & 0 & \pmb{2} & 0 & 0\\
%         0 & 0 & 0 & \pmb{3} & 3
%       \end{bmatrix}
% \end{align}
% Esta última matriz estendida é chamada de \emph{matriz escalonada} do sistema. Desta, temos que \eqref{eq:ex_egauss_exec_sl_1}-\eqref{eq:ex_egauss_exec_sl_4} é equivalente ao seguinte sistema triangular
% \begin{align}
%   -2x_1 - 3x_2 + 2x_3 + 3x_4 &= 10\\
%   3x_2 + 2x_3 + 3x_4 &= 0\\
%   2x_3 &= 0\\
%   3x_4 &= 3.
% \end{align}
% Resolvendo da última equação para a primeira, temos
% \begin{align}
%   x_4 &= 1,\\
%   x_3 &= 0,\\
%   x_2 &= \frac{-2x_3 - 3x_4}{3} = -1,\\
%   x_1 &= \frac{10 + 3x_2 - 3x_3 - 3x_4}{-2} = -2.
% \end{align}

% % \ifisoctave
% % No \verb+GNU Octave+, podemos fazer as computações acima com o seguinte \href{https://github.com/phkonzen/notas/blob/master/src/MatematicaNumerica/cap_sl_direto/dados/ex_egauss_exec/ex_egauss_exec.m}{código}:
% % \verbatiminput{./cap_sl_direto/dados/ex_egauss_exec/ex_egauss_exec.m}
% % \fi
% \end{ex}

% \begin{obs}
%   Para a resolução de um sistema linear $n \times n$, o método de eliminação gaussiana demanda
%   \begin{equation}
%     \frac{n^3}{3} + n^2 - \frac{n}{3}
%   \end{equation}
% multiplicações/divisões e
% \begin{equation}
%   \frac{n^3}{3} + \frac{n^2}{2} - \frac{5n}{6}
% \end{equation}
% adições/subtrações \cite{Burden2015a}.
% \end{obs}

% Com o mesmo custo computacional, podemos utilizar o método de eliminação gaussiana para transformar o sistema dado em um sistema diagonal.

% \begin{ex}\label{ex:egauss_reduzida}
%   Voltando ao exemplo anterior (Exemplo \ref{ex:egauss_exec}, vimos que a matriz estendida do sistema \eqref{eq:ex_egauss_exec_sl_1}-\eqref{eq:ex_egauss_exec_sl_4} é equivalente a
%   \begin{equation}
%     E \sim       
%     \begin{bmatrix}
%         -2 & -3 & 2 & 3 & 10\\
%         0 & 3 & 2 & 3 & 0\\
%         0 & 0 & 2 & 0 & 0\\
%         0 & 0 & 0 & 3 & 3
%       \end{bmatrix}.
%   \end{equation}
% Então, podemos continuar aplicando o método de eliminação gaussiana, agora de baixo para cima, até obtermos um sistema diagonal equivalente. Vejamos
% \begin{align}
%   E &\sim       
%       \begin{bmatrix}
%         -2 & -3 & 2 & 3 & 10\\
%         0 & 3 & 2 & 3 & 0\\
%         0 & 0 & 2 & 0 & 0\\
%         0 & 0 & 0 & \pmb{3} & 3
%       \end{bmatrix}
%       \begin{array}{l}
%       E_1 \leftarrow E_1 - (e_{14}/e_{44})E_4\\
%       E_2 \leftarrow E_2 - (e_{24}/e_{44})E_4\\
%       \\
%       \\
%     \end{array}\\
%     &\sim       
%       \begin{bmatrix}
%         -2 & -3 & 2 & 0 & 7\\
%         0 & 3 & 2 & 0 & -3\\
%         0 & 0 & \pmb{2} & 0 & 0\\
%         0 & 0 & 0 & 3 & 3
%       \end{bmatrix}
%       \begin{array}{l}
%       E_1 \leftarrow E_1 - (e_{13}/e_{33})E_3\\
%       E_2 \leftarrow E_2 - (e_{23}/e_{33})E_3\\
%       \\
%       \\
%     \end{array}\\
%     &\sim       
%       \begin{bmatrix}
%         -2 & -3 & 0 & 0 & 4\\
%         0 & \pmb{3} & 0 & 0 & -3\\
%         0 & 0 & 2 & 0 & 0\\
%         0 & 0 & 0 & 3 & 3
%       \end{bmatrix}
%       \begin{array}{l}
%       E_1 \leftarrow E_1 - (e_{12}/e_{22})E_2\\
%       \\
%       \\
%       \\
%     \end{array}\\
%     &\sim       
%       \begin{bmatrix}
%         \pmb{-2} & 0 & 0 & 0 & 4\\
%         0 & \pmb{3} & 0 & 0 & -3\\
%         0 & 0 & \pmb{2} & 0 & 0\\
%         0 & 0 & 0 & \pmb{3} & 3
%       \end{bmatrix}
%       \begin{array}{l}
%       E_1 \leftarrow E_1/e_{11}\\
%       E_2 \leftarrow E_2/e_{22}\\
%       E_3 \leftarrow E_3/e_{33}\\
%       E_4 \leftarrow E_4/e_{44}\\
%     \end{array}\\
%     &\sim       
%       \begin{bmatrix}
%         1 & 0 & 0 & 0 & -2\\
%         0 & 1 & 0 & 0 & -1\\
%         0 & 0 & 1 & 0 & 0\\
%         0 & 0 & 0 & 1 & 1
%       \end{bmatrix}
%   \end{align}
% Esta última matriz é chamada de matriz escalonada reduzida (por linhas) e a solução do sistema encontra-se em sua última coluna, i.e. $\pmb{x} = (-2, -1, 0, 1)$.

% % \ifisoctave
% % No \verb+GNU Octave+, podemos fazer as computações acima com o seguinte \href{https://github.com/phkonzen/notas/blob/master/src/MatematicaNumerica/cap_sl_direto/dados/ex_egauss_reduzida/ex_egauss_reduzida.m}{código}:
% % \verbatiminput{./cap_sl_direto/dados/ex_egauss_reduzida/ex_egauss_reduzida.m}
% % \fi
% \end{ex}

% \subsection*{Exercícios}

% \begin{exer}\label{exer:egauss_reduzida}
%   Use o método de eliminação gaussiana para obter a matriz escalonada reduzida do seguinte sistema
%   \begin{align}
%     -3x_1 + 2x_2 -5x_4 + x_5 &= -23\\
%     -x_2 -3x_3 &= 9\\
%     -2x_1 -x_2 + x_3 &= -1\\
%     2x_2 - 4x_3 + 3x_4 &= 8\\
%     x_1 - 3x_3 - x_5 &= 11
%   \end{align}
% \end{exer}
% \begin{resp}
%   %   \ifisoctave 
%   % \href{https://github.com/phkonzen/notas/blob/master/src/MatematicaNumerica/cap_sl_direto/dados/exer_egauss_reduzida/exer_egauss_reduzida.m}{Código.} 
%   % \fi
%   $$
%   \begin{bmatrix}
%     1 & 0 & 0 & 0 & 0 & 1\\
%     0 & 1 & 0 & 0 & 0 & -3\\
%     0 & 0 & 1 & 0 & 0 & -2\\
%     0 & 0 & 0 & 1 & 0 & 2\\
%     0 & 0 & 0 & 0 & 1 & -4\\
%   \end{bmatrix}
%   $$
% \end{resp}

% \begin{exer}\label{exer:egauss_arredondamento}
%   Use o método de eliminação gaussiana para obter a matriz escalonada reduzida do seguinte sistema
%   \begin{align}
%     -10^{-12}x_1 + 20x_2 - 3x_3 &= -1\\
%     2,001x_1 + 10^{-5}x_2 - x_3 &= -2\\
%     4x_1 - 2x_2 + x_3 &= 0,1
%   \end{align}
% \end{exer}
% \begin{resp}
%   % \ifisoctave 
%   % \href{https://github.com/phkonzen/notas/blob/master/src/MatematicaNumerica/cap_sl_direto/dados/exer_egauss_arredondamento/exer_egauss_arredondamento.m}{Código.} 
%   % \fi
%   $$
%   \begin{bmatrix}
%    1.0000 &  0.0000 &  0.0000 & -3.9435\E-1\\
%   -0.0000 &  1.0000 & -0.0000 & -2.3179\E-1\\
%    0.0000 &  0.0000 &  1.0000 &  1.2120\E+0
%   \end{bmatrix}
%   $$
% \end{resp}
