%Este trabalho está licenciado sob a Licença Atribuição-CompartilhaIgual 4.0 Internacional Creative Commons. Para visualizar uma cópia desta licença, visite http://creativecommons.org/licenses/by-sa/4.0/deed.pt_BR ou mande uma carta para Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.

\chapter{Sistemas Não Lineares e Otimização}\label{cap_otimizacao}
\thispagestyle{fancy}

\begin{flushright}
  [Vídeo] | [Áudio] | \href{https://phkonzen.github.io/notas/contato.html}{[Contatar]}
\end{flushright}

Neste capítulo, apresentam-se métodos numéricos para a resolução de problemas de otimização. Salvo explicitado ao contrário, assume-se que os problemas são bem definidos.

\section{Método de Newton}\label{cap_otimizacao_sec_newton}

Consideremos o seguinte problema: $F:D\subset \mathbb{R}^n\to\mathbb{R}^n$ encontrar $x^*\in\mathbb{R}^n$ tal que
\begin{equation}
  F(x^*) = 0.
\end{equation}
Salvo explicitado ao contrário, assumiremos que $F\in C^1(D)$, i.e. $F$ é uma função continuamente diferenciável. Vamos, também, denotar por $J_F(x) = [\jmath_{i,j}(x)]_{i,j=1}^{n,n}$ a \emph{matriz jacobiana}\footnote{Carl Gustav Jakob Jacobi, 1804 - 1851, matemático alemão. Fonte: \href{https://pt.wikipedia.org/wiki/Carl_Gustav_Jakob_Jacobi}{Wikipédia}.} com
\begin{equation}
  \jmath_{i,j}(x) = \frac{\p f_i(x)}{\p x_j},
\end{equation}
onde $F(x) = (f_1(x), f_2(x), \dotsc, f_n(x))$ e $x = (x_1, x_2,\dotsc, x_n)$.

A iteração básica do \emph{Método de Newton}\footnote{Isaac Newton, 1642 - 1727, matemático, físico, astrônomo, teólogo e autor inglês. Fonte: \href{https://pt.wikipedia.org/wiki/Isaac_Newton}{Wikipédia}.} consiste em: dada uma aproximação inicial $x^{(0)}\in\mathbb{R}^n$,
\begin{align}
  &\text{resolver}\quad J_F\left(x^{(k)}\right)\delta^{(k)} = -F\left(x^{(k)}\right)\\
  &\text{calcular}\quad x^{(k+1)} = x^{(k)} + \delta^{(k)}
\end{align}
para $k=0,1,2,\ldots$ até convergência $x^{(k)}\to x^*$.

\begin{obs}\label{obs:convNewton}
  Para $x^{(0)}$ suficientemente próximo da solução $x^*$, o Método de Newton é quadraticamente convergente. Mais precisamente, este resultado de convergência local requer que $J_F^{-1}(x^*)$ seja não singular e que $J_F:\mathbb{R}^n\to\mathbb{R}^{n\times n}$ seja Lipschitz\footnote{Rudolf Otto Sigismund Lipschitz, 1832 - 1903, matemático alemão. Fonte: \href{https://en.wikipedia.org/wiki/Rudolf_Lipschitz}{Wikipédia}.} contínua. Consulte \cite[Seção 7.1]{Quarteroni2007} para mais detalhes.
\end{obs}

\begin{ex}\label{ex:burgers}
  Consideremos a \emph{Equação de Burgers}\footnote{Johannes Martinus Burgers, 1895 - 1981, físico holandês. Fonte: \href{https://en.wikipedia.org/wiki/Jan_Burgers}{Wikipedia}.}
  \begin{equation}
    \frac{\p u}{\p t} + u\frac{\p u}{\p x} = \nu\frac{\p^2 u}{\p x^2}
  \end{equation}
  com $\nu>0$, condição inicial
  \begin{equation}
    u(0,x) = \sin(\pi x)
  \end{equation}
  e condições de contorno de Dirichlet\footnote{Johann Peter Gustav Lejeune Dirichlet, 1805 - 1859, matemático alemão. Fonte: \href{https://pt.wikipedia.org/wiki/Johann_Peter_Gustav_Lejeune_Dirichlet}{Wikipédia}.} homogêneas
  \begin{equation}
    u(t,0) = u(t,1) = 0.
  \end{equation}

  Aplicando o \emph{Método de Rothe}\footnote{Erich Hans Rothe, 1895 - 1988, matemático alemão. Fonte: \href{https://pt.wikipedia.org/wiki/Erich_Rothe}{Wikipédia}.} com aproximação de Euler\footnote{Leonhard Paul Euler, 1707 - 1783, matemático e físico suíço. Fonte: \href{https://pt.wikipedia.org/wiki/Leonhard_Euler}{Wikipédia}.} implícita, obtemos
  \begin{equation}
    \frac{u(t+h_t,x) - u(t,x)}{h_t} + u(t+h_t,x)u_x(t+h_t,x) \approx \nu u_{xx}(t+h_t,x),
  \end{equation}
  onde $h_t>0$ é o passo no tempo. Agora, aplicamos diferenças finitas para obter
  \begin{gather}
    \frac{u(t+h_t,x_i) - u(t,x_i)}{h_t} + u(t+h_t, x_i)\frac{u(t+h_t,x_{i+1})-u(t+h_t,x_i)}{h_x} \\
    \approx \nu\frac{u(t+h_t,x_{i-1}) - 2u(t+h_t,x_i) + u(t+h_t,x_{i+1})}{h_x^2},
  \end{gather}
  onde, $x_i=(i-1)h_x$, $i=1,\dotsc,n_x$ e $h_x=1/(n_x-1)$ é o tamanho da malha.

  Rearranjando os termos e denotando $w^{(k)}_i\approx u(t_k, x_i)$, $t_k = (k-1)h$, obtemos o seguinte \emph{problema discreto}
  \begin{gather}
    w^{(k+1)}_1 = 0\\
    ~\nonumber\\
    \frac{1}{h_t}w^{(k+1)}_i - \frac{1}{h_t}w^{(k)}_i + \frac{1}{h_x}w^{(k+1)}_iw^{(k+1)}_{i+1} - \frac{1}{h_x}w^{(k+1)}_iw^{(k+1)}_i \\
    - \frac{\nu}{h_x^2}w^{(k+1)}_{i-1} + 2\frac{\nu}{h_x^2}w^{(k+1)}_i - \frac{\nu}{h_x}w^{(k+1)}_{i+1} = 0,\\
    ~\nonumber\\
    w^{(k+1)}_{n_x} = 0,
  \end{gather}
  sendo $w^{(0)}_i = \sen(\pi x_i)$, $i=1,\dotsc,n_x$ e $k=1,2,\ldots$.

  Este problema pode ser reescrito como segue: para cada $k=1,2,\ldots$, encontrar $v\in\mathbb{R}^{n_x}$, tal que
  \begin{equation}
    F(v; v^{(0)}) = 0, 
  \end{equation}
  onde $v_{i} \approx w^{(k+1)}_i$, $v_{i}^{(0)} \approx w^{(k)}_{i}$ e
  \begin{gather}
    f_1(v,v^{(0)}) = v_1,\\
    f_{i}(v;v^{(0)}) = \frac{1}{h_t}v_i - \frac{1}{h_t}v^{(0)}_i + \frac{1}{h_x}v_iv_{i+1} - \frac{1}{h_x}v_iv_i \\
    - \frac{\nu}{h_x^2}v_{i-1} + 2\frac{\nu}{h_x^2}v_i - \frac{\nu}{h_x}v_{i+1},\\
    f_{n_x}(v;v^{(0)}) = v_{n_x}.
  \end{gather}
  A matriz jacobiana associada $J=[\jmath_{i,j}]_{i,j}^{n_x,n_x}$ contém
  \begin{gather}
    \jmath_{i,j} = 0,\quad j\neq i-1,i,i+1,\\
    \jmath_{1,1} = 1,\\
    \jmath_{1,2} = 0,\\
    \jmath_{i,i-1} = -\frac{\nu}{h_x^2},\\
    \jmath_{i,i} = \frac{1}{h_t} + \frac{1}{h_x}v_{i+1} - \frac{2}{h_x}v_i + \frac{2\nu}{h_x^2},\\
    \jmath_{i,i+1} = \frac{1}{h_x}v_{i} - \frac{\nu}{h_x^2},\\
    \jmath_{n_x,n_x-1} = 0\\
    \jmath_{n_x,n_x} = 1.
  \end{gather}
\end{ex}

\begin{exer}\label{exer:burgers}
  Considere o problema discreto apresentado no Exemplo \ref{ex:burgers} para diferentes valores do coeficiente de difusão $\nu=1., 0.5, 0.1, 0.01, 0.001$. Simule o problema com cada uma das seguintes estratégias e as compare quanto ao desempenho computacional.
  \begin{enumerate}[a)]
  \item Simule-o aplicando o Método de Newton com o {\it solver} linear \lstinline+npla.solve+.
  \item Observe que a jacobiana é uma matriz tridiagonal. Simule o problema aplicando o Método de Newton com o {\it solver} linear \lstinline+npla.solve_banded+.
  \item Aloque a jacobiana como uma matriz esparsa. Então, simule o problema aplicando o Método de Newton com {\it solver} linear adequado para matrizes esparsas.
  \end{enumerate}
\end{exer}

\section{Método Tipo Newton}\label{cap_otimizacao_sec_tipoNewton}

Existem várias modificações do Método de Newton\footnote{Isaac Newton, 1642 - 1727, matemático, físico, astrônomo, teólogo e autor inglês. Fonte: \href{https://pt.wikipedia.org/wiki/Isaac_Newton}{Wikipédia}.} que buscam reduzir o custo computacional. Há estratégias para simplificar as computações da matriz jacobiana\footnote{Carl Gustav Jakob Jacobi, 1804 - 1851, matemático alemão. Fonte: \href{https://pt.wikipedia.org/wiki/Carl_Gustav_Jakob_Jacobi}{Wikipédia}.} e para reduzir o custo nas computações das atualizações de Newton.

\subsection{Atualização Cíclica da Matriz Jacobiana}

Geralmente, ao simplificarmos a matriz jacobina $J_F$ ou aproximarmos a atualização de Newton $\delta^{(k)}$, perdemos a convergência quadrática do método (consulte a Observação \ref{obs:convNewton}). A ideia é, então, buscar uma convergência pelo menos super-linear, i.e.
\begin{equation}
  \|e^{(k+1)}\|\approx \rho_k\|e^{(k)}\|,
\end{equation}
com $\rho_k\to 0$ quando $k\to\infty$. Aqui, $e^{(k)}:=x^*-x^{(k)}$. Se a convergência é superlinear, então podemos usar a seguinte aproximação
\begin{equation}
  \rho_k \approx \frac{\|\delta^{(k)}\|}{\|\delta^{(k-1)}\|}
\end{equation}
ou, equivalentemente,
\begin{equation}
  \rho_k \approx \left(\frac{\|\delta^{(k)}\|}{\|\delta^{(0)}\|}\right)^{\frac{1}{k}}
\end{equation}
Isso mostra que podemos acompanhar a convergência das iterações pelo fator
\begin{equation}
  \beta_k = \frac{\|\delta^{(k)}\|}{\|\delta^{(0)}\|}.
\end{equation}
Ao garantirmos $0<\beta_k<1$, deveremos ter uma convergência superlinear.

Vamos, então, propor o seguinte  método tipo Newton de atualização cíclica da matriz jacobiana.
\begin{enumerate}
\item Escolha $0<\beta<1$
\item $J := J_F(x^{(0)})$
\item $J\delta^{(0)}=-F(x^{(0)})$
\item $x^{(1)} = x^{(0)} + \delta^{(0)}$
\item Para $k=1,\ldots$ até critério de convergência:
  \begin{enumerate}
  \item $J\delta^{(k)}=-F(x^{(k)})$
  \item $x^{(k+1)}=x^{(k)} + \delta^{(k)}$
  \item Se $\|\delta^{(k)}\|/\|\delta^{(0)}\| > \beta$:
    \begin{enumerate}
    \item $J := J_F(x^{(k)})$
    \end{enumerate}
  \end{enumerate}
\end{enumerate}

\begin{exer}
  Implemente uma versão do Método Tipo Newton apresentado acima e aplique-o para simular o problema discutido no Exemplo \ref{ex:burgers} para $\nu = 1., 0.1, 0.01, 0.001, 0.0001$. Faça uma implementação com suporte para matrizes esparsas.
\end{exer}

\section{Problemas de Minimização}\label{cap_otimizacao_sec_minimi}

Vamos considerar o seguinte \emph{problema de minimização}: dada a \emph{função objetivo} $f:D\subset \mathbb{R}^n\to\mathbb{R}$, resolver
\begin{equation}
  \min_{x\in D} f(x).
\end{equation}
No que segue e salvo dito explicitamente ao contrário, vamos assumir que o problema está bem determinado e que $f$ é suficientemente suave. Ainda, vamos assumir as seguintes notações:
\begin{itemize}
\item gradiente de $f$
  \begin{equation}
    \nabla f(x) = \left(\frac{\p f}{\p x_1}(x),\dotsc,\frac{\p f}{\p x_n}(x)\right)
  \end{equation}
\item derivada direcional de $f$ com respeito a $d\in\mathbb{R}^n$
  \begin{equation}
    \frac{\p f}{\p d} = \nabla f(x)\cdot d
  \end{equation}
\item matriz hessiana de f, $H=[h_{i,j}]_{i,j=1}^{n,n}$
  \begin{equation}
    h_{i,j}(x) = \frac{\p^2 f}{\p x_i\p x_j}
  \end{equation}
\end{itemize}

\begin{obs}[Condições de Otimização]
  Se $\nabla f(x^*) = 0$ e $H(x^*)$ é positiva definida, então $x^*$ é um mínimo local de $f$ em uma vizinhança não vazia de $x^*$. Consulte mais em \cite[Seção 7.2]{Quarteroni2007}. Um ponto $x^*$ tal que $\nabla f(x^*) = 0$ é chamado de \emph{ponto crítico}.
\end{obs}

\subsection{Métodos de descida}

Um método de descida consiste em uma iteração tal que: dada uma aproximação inicial $x^{(0)}\in\mathbb{R}^n$, computa-se
\begin{equation}
  x^{(k+1)} = x^{(k)} + \alpha_k d^{(k)},
\end{equation}
com \emph{tamanho de passo} $\alpha_k>0$, para $k=0,1,2,\ldots$ até convergência. As \emph{direções descendentes} são tais que
\begin{align}
  &d^{(k)}\cdot\nabla f(x^{(k)}) < 0,\quad \text{se } \nabla f(x^{(k)}) \neq 0,\label{eq:condDirecoes0}\\
  &d^{(k)} = 0,\quad \text{noutro caso.}\label{eq:condDirecoes1}
\end{align}

\begin{obs}(Condição de Convergência)
  Da Série de Taylor\footnote{Brook Taylor, 1685 - 1731, matemático britânico. Fonte: \href{https://pt.wikipedia.org/wiki/Brook_Taylor}{Wikipédia}.}, temos que
  \begin{equation}
    f(x+\alpha_kd^{(k)}) - f(x^{(k)}) = \alpha_k\nabla f(x^{(k)})\cdot d^{(k)} + \varepsilon,
  \end{equation}
  com $\varepsilon\to 0$ quando $\alpha_k\to 0$. Como consequência da continuidade da $f$, para $\alpha_k$ suficientemente pequeno, o sinal do lado esquerdo é igual a do direito desta última equação. Logo, para tais $\alpha_k$ e $d^{(k)}\neq 0$ uma direção descendente temos garantido que
  \begin{equation}
    f(x^{(k)}+\alpha_kd^{(k)}) < f(x^{(k)}).
  \end{equation}
\end{obs}


Notamos que um método de descida fica determinado pelas escolhas da direção de descida $d_k$ e o tamanho do passo $\alpha_k$. Primeiramente, vamos a este último item.

\subsubsection{Pesquisa linear}

A computação de $\alpha_k$ consiste em resolver o seguinte problema de minimização:
\begin{equation}
  \min_{\alpha\in\mathbb{R}} \phi(\alpha) = f(x^{(k)}+\alpha d^{(k)}).
\end{equation}
Entretanto, a resolução exata deste problema é muitas vezes não factível. Técnicas de aproximações para a resolução deste problema são, então, aplicadas. Tais técnicas são chamadas de \emph{pesquisa linear não exata}.

A \emph{condição de Armijo} é que a escolha de $\alpha_k$ deve ser tal que
\begin{equation} \label{eq:condArmijo}
  f(x^{(k)}+\alpha_k d^{(k)}) \leq f(x^{(k)}) + \sigma \alpha \nabla f(x^{(k)})\cdot d^{(k)},
\end{equation}
para alguma constante $\sigma\in (0, 1)$. Ou seja, a redução em $f$ é esperada ser proporcional à derivada direcional de $f$ com relação a direção $d^{(k)}$ no ponto $x^{(k)}$. Em aplicações computacionais, $\sigma$ é normalmente escolhido no intervalo $[10^{-5}, 10^{-1}]$. 

A condição \eqref{eq:condArmijo} não é suficiente para evitar escolhas muito pequenas de $\alpha_k$. Para tanto, pode-se empregar a \emph{condição de curvatura}, a qual requer que
\begin{equation}\label{eq:condCurvatura}
  \nabla f(x^{(k)} + \alpha_kd^{(k)})\cdot d^{(k)} \geq \beta\nabla f(x^{(k)})d^{(k)},
\end{equation}
para $\beta\in [\sigma, 1/2]$. Notemos que o lado esquerdo de \eqref{eq:condCurvatura} é igual a $\phi'(\alpha_k)$. Ou seja, este condição impõe que $\alpha_k$ seja maior que $\beta\phi'(0)$. Normalmente, escolhe-se $\beta\in 10^{-1}, 1/2]$. Juntas, \eqref{eq:condArmijo} e \eqref{eq:condCurvatura} são conhecidas como \emph{condições de Wolfe}\footnote{Philip Wolfe, 1927 - 2016, matemático estadunidense. Fonte: \href{https://pt.wikipedia.org/wiki/Philip_Wolfe}{Wikipédia}.}.

\subsection{Método do Gradiente}

O Método do Gradiente (\emph{Método do Máximo Declive}) é um Método de Descida tal que as direções descendentes são o oposto do gradiente da $f$, i.e.
\begin{equation}
  d^{(k)} = -\nabla f(x^{(k)}).
\end{equation}
É fácil verificar que as condições \eqref{eq:condDirecoes0}-\eqref{eq:condDirecoes1} são satisfeitas.

\begin{ex}\label{ex:Rosenbrock}
  Consideremos o problema de encontrar o mínimo da \emph{função de Rosenbrock}\footnote{Howard Harry Rosenbrock, 1920 - 2010, engenheiro britânico. Fonte: \href{https://en.wikipedia.org/wiki/Howard_Harry_Rosenbrock}{Wikipedia}.}
  \begin{equation}
    f(x) = \sum_{i=1}^n 100(x_{i+1}-x_i^2)^2 + (1-x_i)^2.
  \end{equation}
  O valor mínimo desta função é zero e ocorre no ponto $x=0$. Esta função é comumente usada como caso padrão para teste de métodos de otimização.

  Para o Método do Gradiente, precisamos de suas derivadas parciais:
  \begin{gather}
    \frac{\p f}{\p x_1} = -400x_1(x_2-x_1^2) - 2(1-x_1)\\
    ~\nonumber\\
    \frac{\p f}{\p x_j} = \sum_{i=1}^n 200(x_i-x_{i-1}^2)(\delta_{i,j}-2x_{i-1}\delta_{i-1,j})-2(1-x_{i-1})\delta_{i-1,j}\\
    ~\nonumber\\
    \frac{\p f}{\p x_{n}} = 200(x_n - x_{n-1}^2
  \end{gather}

  \lstinputlisting[caption=Algoritmo do Gradiente, label={lst:algOtimMG}]{./cap_otimizacao/dados/pyMG/main.py}
\end{ex}

\begin{exer}
  Aplique o Método do Gradiente para resolver o problema de minimização com as seguintes funções objetivos:
  \begin{enumerate}[a)]
  \item Função de Beale
    \begin{equation}
      f(x,y) = (1.5-x-xy)^2 + (2.25-x+xy^2)^2 + (2.625-x+xy^3)^2
    \end{equation}
    Solução: f(3, 0.5) = 0.
  \item Função de Goldstein-Price
    \begin{align}
      f(x,y) &= \left[1+\left(x+y+1\right)^{2}\left(19-14x+3x^{2}-14y+6xy+3y^{2}\right)\right]\nonumber\\
      &\times \left[30+\left(2x-3y\right)^{2}\left(18-32x+12x^{2}+48y-36xy+27y^{2}\right)\right]
    \end{align}
    Solução: f(0,-1) = 3.
  \item Função de Booth
    \begin{equation}
      f(x,y) = \left( x + 2y -7\right)^{2} + \left(2x +y - 5\right)^{2}
    \end{equation}
    Solução: f(1,3)=0.
  \item Função de Rastringin:
    \begin{equation}
      f(x) = 10 n + \sum_{i=1}^n \left[x_i^2 - 10\cos(2 \pi x_i)\right]
    \end{equation}
    Solução: f(0)=0.
  \end{enumerate}
\end{exer}





